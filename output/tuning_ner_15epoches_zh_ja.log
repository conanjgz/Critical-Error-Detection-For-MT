| 2021-07-13 07:45:10 | INFO | 
==============================Start training==============================
| 2021-07-13 07:45:10 | INFO | Command Line Args:   --lr 2e-5 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.2
Defaults:
  --hidden_size:     256

| 2021-07-13 07:45:10 | INFO | 
lr: 2e-05

| 2021-07-13 07:45:26 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 07:45:26 | INFO | Start epoch 1:
| 2021-07-13 07:45:27 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 07:46:29 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 07:46:29 | INFO | Validation loss: 0.399, acc: 0.858, F1: 0.462
| 2021-07-13 07:46:29 | INFO | Start epoch 2:
| 2021-07-13 07:46:30 | INFO | Train Loss: 0.409, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 07:47:34 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 07:47:34 | INFO | Validation loss: 0.402, acc: 0.858, F1: 0.462
| 2021-07-13 07:47:34 | INFO | Start epoch 3:
| 2021-07-13 07:47:34 | INFO | Train Loss: 0.419, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 07:48:38 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 07:48:38 | INFO | Validation loss: 0.393, acc: 0.858, F1: 0.462
| 2021-07-13 07:48:38 | INFO | Start epoch 4:
| 2021-07-13 07:48:39 | INFO | Train Loss: 0.527, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 07:49:42 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 07:49:42 | INFO | Validation loss: 0.367, acc: 0.858, F1: 0.462
| 2021-07-13 07:49:42 | INFO | Start epoch 5:
| 2021-07-13 07:49:43 | INFO | Train Loss: 0.364, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 07:50:46 | INFO | Validation tp: 39, fn: 103, fp: 53, tn: 806
| 2021-07-13 07:50:46 | INFO | Validation loss: 0.407, acc: 0.844, F1: 0.623
| 2021-07-13 07:50:46 | INFO | Start epoch 6:
| 2021-07-13 07:50:47 | INFO | Train Loss: 0.308, tp: 7, fn: 6, fp: 2, tn: 49, Acc: 0.875, Prec: 0.778, Rec: 0.538, F1: 0.780
| 2021-07-13 07:51:51 | INFO | Validation tp: 1, fn: 141, fp: 2, tn: 857
| 2021-07-13 07:51:51 | INFO | Validation loss: 0.411, acc: 0.857, F1: 0.468
| 2021-07-13 07:51:51 | INFO | Start epoch 7:
| 2021-07-13 07:51:52 | INFO | Train Loss: 0.426, tp: 0, fn: 14, fp: 1, tn: 49, Acc: 0.766, Prec: 0.000, Rec: 0.000, F1: 0.434
| 2021-07-13 07:52:56 | INFO | Validation tp: 19, fn: 123, fp: 24, tn: 835
| 2021-07-13 07:52:56 | INFO | Validation loss: 0.441, acc: 0.853, F1: 0.562
| 2021-07-13 07:52:56 | INFO | Start epoch 8:
| 2021-07-13 07:52:56 | INFO | Train Loss: 0.260, tp: 3, fn: 8, fp: 0, tn: 53, Acc: 0.875, Prec: 1.000, Rec: 0.273, F1: 0.679
| 2021-07-13 07:54:00 | INFO | Validation tp: 27, fn: 115, fp: 55, tn: 804
| 2021-07-13 07:54:00 | INFO | Validation loss: 0.482, acc: 0.830, F1: 0.573
| 2021-07-13 07:54:00 | INFO | Start epoch 9:
| 2021-07-13 07:54:01 | INFO | Train Loss: 0.174, tp: 5, fn: 1, fp: 4, tn: 54, Acc: 0.922, Prec: 0.556, Rec: 0.833, F1: 0.811
| 2021-07-13 07:55:04 | INFO | Validation tp: 30, fn: 112, fp: 47, tn: 812
| 2021-07-13 07:55:04 | INFO | Validation loss: 0.541, acc: 0.841, F1: 0.592
| 2021-07-13 07:55:04 | INFO | Start epoch 10:
| 2021-07-13 07:55:05 | INFO | Train Loss: 0.200, tp: 12, fn: 3, fp: 0, tn: 49, Acc: 0.953, Prec: 1.000, Rec: 0.800, F1: 0.930
| 2021-07-13 07:56:09 | INFO | Validation tp: 27, fn: 115, fp: 48, tn: 811
| 2021-07-13 07:56:09 | INFO | Validation loss: 0.614, acc: 0.837, F1: 0.579
| 2021-07-13 07:56:09 | INFO | Start epoch 11:
| 2021-07-13 07:56:10 | INFO | Train Loss: 0.097, tp: 8, fn: 2, fp: 1, tn: 53, Acc: 0.953, Prec: 0.889, Rec: 0.800, F1: 0.907
| 2021-07-13 07:57:14 | INFO | Validation tp: 37, fn: 105, fp: 65, tn: 794
| 2021-07-13 07:57:14 | INFO | Validation loss: 0.624, acc: 0.830, F1: 0.603
| 2021-07-13 07:57:14 | INFO | Start epoch 12:
| 2021-07-13 07:57:14 | INFO | Train Loss: 0.054, tp: 11, fn: 1, fp: 0, tn: 52, Acc: 0.984, Prec: 1.000, Rec: 0.917, F1: 0.973
| 2021-07-13 07:58:18 | INFO | Validation tp: 23, fn: 119, fp: 51, tn: 808
| 2021-07-13 07:58:18 | INFO | Validation loss: 0.647, acc: 0.830, F1: 0.559
| 2021-07-13 07:58:18 | INFO | Start epoch 13:
| 2021-07-13 07:58:18 | INFO | Train Loss: 0.150, tp: 10, fn: 1, fp: 1, tn: 52, Acc: 0.969, Prec: 0.909, Rec: 0.909, F1: 0.945
| 2021-07-13 07:59:22 | INFO | Validation tp: 29, fn: 113, fp: 46, tn: 813
| 2021-07-13 07:59:22 | INFO | Validation loss: 0.703, acc: 0.841, F1: 0.589
| 2021-07-13 07:59:22 | INFO | Start epoch 14:
| 2021-07-13 07:59:23 | INFO | Train Loss: 0.121, tp: 18, fn: 1, fp: 0, tn: 45, Acc: 0.984, Prec: 1.000, Rec: 0.947, F1: 0.981
| 2021-07-13 08:00:27 | INFO | Validation tp: 33, fn: 109, fp: 48, tn: 811
| 2021-07-13 08:00:27 | INFO | Validation loss: 0.717, acc: 0.843, F1: 0.604
| 2021-07-13 08:00:27 | INFO | Start epoch 15:
| 2021-07-13 08:00:28 | INFO | Train Loss: 0.049, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 08:01:32 | INFO | Validation tp: 27, fn: 115, fp: 38, tn: 821
| 2021-07-13 08:01:32 | INFO | Validation loss: 0.704, acc: 0.847, F1: 0.588
| 2021-07-13 08:01:34 | INFO | 
==============================Start training==============================
| 2021-07-13 08:01:34 | INFO | Command Line Args:   --lr 3e-5 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.2
Defaults:
  --hidden_size:     256

| 2021-07-13 08:01:34 | INFO | 
lr: 3e-05

| 2021-07-13 08:01:51 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 08:01:51 | INFO | Start epoch 1:
| 2021-07-13 08:01:51 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 08:02:55 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:02:55 | INFO | Validation loss: 0.408, acc: 0.858, F1: 0.462
| 2021-07-13 08:02:55 | INFO | Start epoch 2:
| 2021-07-13 08:02:55 | INFO | Train Loss: 0.376, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 08:03:59 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:03:59 | INFO | Validation loss: 0.381, acc: 0.858, F1: 0.462
| 2021-07-13 08:03:59 | INFO | Start epoch 3:
| 2021-07-13 08:04:00 | INFO | Train Loss: 0.403, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 08:05:04 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:05:04 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 08:05:04 | INFO | Start epoch 4:
| 2021-07-13 08:05:05 | INFO | Train Loss: 0.519, tp: 2, fn: 16, fp: 0, tn: 46, Acc: 0.750, Prec: 1.000, Rec: 0.111, F1: 0.526
| 2021-07-13 08:06:09 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:06:09 | INFO | Validation loss: 0.372, acc: 0.858, F1: 0.462
| 2021-07-13 08:06:09 | INFO | Start epoch 5:
| 2021-07-13 08:06:10 | INFO | Train Loss: 0.360, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 08:07:14 | INFO | Validation tp: 26, fn: 116, fp: 33, tn: 826
| 2021-07-13 08:07:14 | INFO | Validation loss: 0.390, acc: 0.851, F1: 0.588
| 2021-07-13 08:07:14 | INFO | Start epoch 6:
| 2021-07-13 08:07:14 | INFO | Train Loss: 0.348, tp: 5, fn: 8, fp: 1, tn: 50, Acc: 0.859, Prec: 0.833, Rec: 0.385, F1: 0.722
| 2021-07-13 08:08:18 | INFO | Validation tp: 1, fn: 141, fp: 5, tn: 854
| 2021-07-13 08:08:18 | INFO | Validation loss: 0.414, acc: 0.854, F1: 0.467
| 2021-07-13 08:08:18 | INFO | Start epoch 7:
| 2021-07-13 08:08:19 | INFO | Train Loss: 0.512, tp: 2, fn: 12, fp: 0, tn: 50, Acc: 0.812, Prec: 1.000, Rec: 0.143, F1: 0.571
| 2021-07-13 08:09:23 | INFO | Validation tp: 19, fn: 123, fp: 28, tn: 831
| 2021-07-13 08:09:23 | INFO | Validation loss: 0.456, acc: 0.849, F1: 0.559
| 2021-07-13 08:09:23 | INFO | Start epoch 8:
| 2021-07-13 08:09:23 | INFO | Train Loss: 0.338, tp: 3, fn: 8, fp: 0, tn: 53, Acc: 0.875, Prec: 1.000, Rec: 0.273, F1: 0.679
| 2021-07-13 08:10:27 | INFO | Validation tp: 34, fn: 108, fp: 71, tn: 788
| 2021-07-13 08:10:27 | INFO | Validation loss: 0.422, acc: 0.821, F1: 0.587
| 2021-07-13 08:10:27 | INFO | Start epoch 9:
| 2021-07-13 08:10:28 | INFO | Train Loss: 0.241, tp: 4, fn: 2, fp: 2, tn: 56, Acc: 0.938, Prec: 0.667, Rec: 0.667, F1: 0.816
| 2021-07-13 08:11:32 | INFO | Validation tp: 25, fn: 117, fp: 43, tn: 816
| 2021-07-13 08:11:32 | INFO | Validation loss: 0.503, acc: 0.840, F1: 0.574
| 2021-07-13 08:11:32 | INFO | Start epoch 10:
| 2021-07-13 08:11:33 | INFO | Train Loss: 0.256, tp: 9, fn: 6, fp: 0, tn: 49, Acc: 0.906, Prec: 1.000, Rec: 0.600, F1: 0.846
| 2021-07-13 08:12:37 | INFO | Validation tp: 26, fn: 116, fp: 44, tn: 815
| 2021-07-13 08:12:37 | INFO | Validation loss: 0.502, acc: 0.840, F1: 0.578
| 2021-07-13 08:12:37 | INFO | Start epoch 11:
| 2021-07-13 08:12:37 | INFO | Train Loss: 0.182, tp: 6, fn: 4, fp: 2, tn: 52, Acc: 0.906, Prec: 0.750, Rec: 0.600, F1: 0.806
| 2021-07-13 08:13:42 | INFO | Validation tp: 43, fn: 99, fp: 77, tn: 782
| 2021-07-13 08:13:42 | INFO | Validation loss: 0.566, acc: 0.824, F1: 0.614
| 2021-07-13 08:13:42 | INFO | Start epoch 12:
| 2021-07-13 08:13:42 | INFO | Train Loss: 0.161, tp: 12, fn: 0, fp: 3, tn: 49, Acc: 0.953, Prec: 0.800, Rec: 1.000, F1: 0.930
| 2021-07-13 08:14:46 | INFO | Validation tp: 21, fn: 121, fp: 23, tn: 836
| 2021-07-13 08:14:46 | INFO | Validation loss: 0.572, acc: 0.856, F1: 0.573
| 2021-07-13 08:14:46 | INFO | Start epoch 13:
| 2021-07-13 08:14:47 | INFO | Train Loss: 0.168, tp: 7, fn: 4, fp: 1, tn: 52, Acc: 0.922, Prec: 0.875, Rec: 0.636, F1: 0.845
| 2021-07-13 08:15:51 | INFO | Validation tp: 44, fn: 98, fp: 73, tn: 786
| 2021-07-13 08:15:51 | INFO | Validation loss: 0.654, acc: 0.829, F1: 0.621
| 2021-07-13 08:15:51 | INFO | Start epoch 14:
| 2021-07-13 08:15:52 | INFO | Train Loss: 0.079, tp: 16, fn: 3, fp: 0, tn: 45, Acc: 0.953, Prec: 1.000, Rec: 0.842, F1: 0.941
| 2021-07-13 08:16:56 | INFO | Validation tp: 29, fn: 113, fp: 42, tn: 817
| 2021-07-13 08:16:56 | INFO | Validation loss: 0.645, acc: 0.845, F1: 0.593
| 2021-07-13 08:16:56 | INFO | Start epoch 15:
| 2021-07-13 08:16:56 | INFO | Train Loss: 0.069, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 08:18:00 | INFO | Validation tp: 28, fn: 114, fp: 42, tn: 817
| 2021-07-13 08:18:00 | INFO | Validation loss: 0.658, acc: 0.844, F1: 0.589
| 2021-07-13 08:18:03 | INFO | 
==============================Start training==============================
| 2021-07-13 08:18:03 | INFO | Command Line Args:   --lr 4e-5 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.2
Defaults:
  --hidden_size:     256

| 2021-07-13 08:18:03 | INFO | 
lr: 4e-05

| 2021-07-13 08:18:19 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 08:18:19 | INFO | Start epoch 1:
| 2021-07-13 08:18:20 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 08:19:24 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:19:24 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 08:19:24 | INFO | Start epoch 2:
| 2021-07-13 08:19:24 | INFO | Train Loss: 0.400, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 08:20:28 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:20:28 | INFO | Validation loss: 0.384, acc: 0.858, F1: 0.462
| 2021-07-13 08:20:28 | INFO | Start epoch 3:
| 2021-07-13 08:20:29 | INFO | Train Loss: 0.414, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 08:21:33 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:21:33 | INFO | Validation loss: 0.375, acc: 0.858, F1: 0.462
| 2021-07-13 08:21:33 | INFO | Start epoch 4:
| 2021-07-13 08:21:34 | INFO | Train Loss: 0.492, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 08:22:38 | INFO | Validation tp: 12, fn: 130, fp: 13, tn: 846
| 2021-07-13 08:22:38 | INFO | Validation loss: 0.410, acc: 0.857, F1: 0.533
| 2021-07-13 08:22:38 | INFO | Start epoch 5:
| 2021-07-13 08:22:38 | INFO | Train Loss: 0.412, tp: 2, fn: 8, fp: 2, tn: 52, Acc: 0.844, Prec: 0.500, Rec: 0.200, F1: 0.599
| 2021-07-13 08:23:42 | INFO | Validation tp: 30, fn: 112, fp: 51, tn: 808
| 2021-07-13 08:23:42 | INFO | Validation loss: 0.408, acc: 0.837, F1: 0.589
| 2021-07-13 08:23:42 | INFO | Start epoch 6:
| 2021-07-13 08:23:43 | INFO | Train Loss: 0.302, tp: 7, fn: 6, fp: 4, tn: 47, Acc: 0.844, Prec: 0.636, Rec: 0.538, F1: 0.744
| 2021-07-13 08:24:47 | INFO | Validation tp: 21, fn: 121, fp: 44, tn: 815
| 2021-07-13 08:24:47 | INFO | Validation loss: 0.483, acc: 0.835, F1: 0.555
| 2021-07-13 08:24:47 | INFO | Start epoch 7:
| 2021-07-13 08:24:47 | INFO | Train Loss: 0.277, tp: 10, fn: 4, fp: 1, tn: 49, Acc: 0.922, Prec: 0.909, Rec: 0.714, F1: 0.876
| 2021-07-13 08:25:51 | INFO | Validation tp: 18, fn: 124, fp: 41, tn: 818
| 2021-07-13 08:25:51 | INFO | Validation loss: 0.548, acc: 0.835, F1: 0.544
| 2021-07-13 08:25:51 | INFO | Start epoch 8:
| 2021-07-13 08:25:52 | INFO | Train Loss: 0.145, tp: 9, fn: 2, fp: 0, tn: 53, Acc: 0.969, Prec: 1.000, Rec: 0.818, F1: 0.941
| 2021-07-13 08:26:56 | INFO | Validation tp: 18, fn: 124, fp: 38, tn: 821
| 2021-07-13 08:26:56 | INFO | Validation loss: 0.592, acc: 0.838, F1: 0.546
| 2021-07-13 08:26:56 | INFO | Start epoch 9:
| 2021-07-13 08:26:56 | INFO | Train Loss: 0.056, tp: 5, fn: 1, fp: 0, tn: 58, Acc: 0.984, Prec: 1.000, Rec: 0.833, F1: 0.950
| 2021-07-13 08:28:01 | INFO | Validation tp: 27, fn: 115, fp: 67, tn: 792
| 2021-07-13 08:28:01 | INFO | Validation loss: 0.683, acc: 0.818, F1: 0.563
| 2021-07-13 08:28:01 | INFO | Start epoch 10:
| 2021-07-13 08:28:01 | INFO | Train Loss: 0.124, tp: 13, fn: 2, fp: 0, tn: 49, Acc: 0.969, Prec: 1.000, Rec: 0.867, F1: 0.954
| 2021-07-13 08:29:06 | INFO | Validation tp: 34, fn: 108, fp: 88, tn: 771
| 2021-07-13 08:29:06 | INFO | Validation loss: 0.762, acc: 0.804, F1: 0.572
| 2021-07-13 08:29:06 | INFO | Start epoch 11:
| 2021-07-13 08:29:06 | INFO | Train Loss: 0.039, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 08:30:10 | INFO | Validation tp: 31, fn: 111, fp: 83, tn: 776
| 2021-07-13 08:30:10 | INFO | Validation loss: 0.810, acc: 0.806, F1: 0.566
| 2021-07-13 08:30:10 | INFO | Start epoch 12:
| 2021-07-13 08:30:11 | INFO | Train Loss: 0.057, tp: 11, fn: 1, fp: 2, tn: 50, Acc: 0.953, Prec: 0.846, Rec: 0.917, F1: 0.925
| 2021-07-13 08:31:15 | INFO | Validation tp: 31, fn: 111, fp: 73, tn: 786
| 2021-07-13 08:31:15 | INFO | Validation loss: 0.820, acc: 0.816, F1: 0.574
| 2021-07-13 08:31:15 | INFO | Start epoch 13:
| 2021-07-13 08:31:15 | INFO | Train Loss: 0.091, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-13 08:32:19 | INFO | Validation tp: 28, fn: 114, fp: 60, tn: 799
| 2021-07-13 08:32:19 | INFO | Validation loss: 0.898, acc: 0.826, F1: 0.573
| 2021-07-13 08:32:19 | INFO | Start epoch 14:
| 2021-07-13 08:32:20 | INFO | Train Loss: 0.006, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 08:33:24 | INFO | Validation tp: 22, fn: 120, fp: 51, tn: 808
| 2021-07-13 08:33:24 | INFO | Validation loss: 0.892, acc: 0.829, F1: 0.554
| 2021-07-13 08:33:24 | INFO | Start epoch 15:
| 2021-07-13 08:33:25 | INFO | Train Loss: 0.032, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 08:34:29 | INFO | Validation tp: 30, fn: 112, fp: 64, tn: 795
| 2021-07-13 08:34:29 | INFO | Validation loss: 0.918, acc: 0.824, F1: 0.577
| 2021-07-13 08:34:31 | INFO | 
==============================Start training==============================
| 2021-07-13 08:34:31 | INFO | Command Line Args:   --lr 5e-5 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.2
Defaults:
  --hidden_size:     256

| 2021-07-13 08:34:31 | INFO | 
lr: 5e-05

| 2021-07-13 08:34:48 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 08:34:48 | INFO | Start epoch 1:
| 2021-07-13 08:34:48 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 08:35:52 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:35:52 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 08:35:52 | INFO | Start epoch 2:
| 2021-07-13 08:35:52 | INFO | Train Loss: 0.411, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 08:36:56 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:36:56 | INFO | Validation loss: 0.403, acc: 0.858, F1: 0.462
| 2021-07-13 08:36:56 | INFO | Start epoch 3:
| 2021-07-13 08:36:57 | INFO | Train Loss: 0.431, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 08:38:01 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:38:01 | INFO | Validation loss: 0.377, acc: 0.858, F1: 0.462
| 2021-07-13 08:38:01 | INFO | Start epoch 4:
| 2021-07-13 08:38:01 | INFO | Train Loss: 0.511, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 08:39:06 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:39:06 | INFO | Validation loss: 0.398, acc: 0.858, F1: 0.462
| 2021-07-13 08:39:06 | INFO | Start epoch 5:
| 2021-07-13 08:39:06 | INFO | Train Loss: 0.439, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 08:40:10 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:40:10 | INFO | Validation loss: 0.375, acc: 0.858, F1: 0.462
| 2021-07-13 08:40:10 | INFO | Start epoch 6:
| 2021-07-13 08:40:11 | INFO | Train Loss: 0.348, tp: 1, fn: 12, fp: 0, tn: 51, Acc: 0.812, Prec: 1.000, Rec: 0.077, F1: 0.519
| 2021-07-13 08:41:14 | INFO | Validation tp: 7, fn: 135, fp: 8, tn: 851
| 2021-07-13 08:41:14 | INFO | Validation loss: 0.395, acc: 0.857, F1: 0.506
| 2021-07-13 08:41:14 | INFO | Start epoch 7:
| 2021-07-13 08:41:15 | INFO | Train Loss: 0.411, tp: 2, fn: 12, fp: 1, tn: 49, Acc: 0.797, Prec: 0.667, Rec: 0.143, F1: 0.559
| 2021-07-13 08:42:19 | INFO | Validation tp: 14, fn: 128, fp: 21, tn: 838
| 2021-07-13 08:42:19 | INFO | Validation loss: 0.420, acc: 0.851, F1: 0.538
| 2021-07-13 08:42:19 | INFO | Start epoch 8:
| 2021-07-13 08:42:19 | INFO | Train Loss: 0.265, tp: 3, fn: 8, fp: 0, tn: 53, Acc: 0.875, Prec: 1.000, Rec: 0.273, F1: 0.679
| 2021-07-13 08:43:23 | INFO | Validation tp: 47, fn: 95, fp: 101, tn: 758
| 2021-07-13 08:43:23 | INFO | Validation loss: 0.447, acc: 0.804, F1: 0.605
| 2021-07-13 08:43:23 | INFO | Start epoch 9:
| 2021-07-13 08:43:24 | INFO | Train Loss: 0.215, tp: 6, fn: 0, fp: 4, tn: 54, Acc: 0.938, Prec: 0.600, Rec: 1.000, F1: 0.857
| 2021-07-13 08:44:28 | INFO | Validation tp: 44, fn: 98, fp: 89, tn: 770
| 2021-07-13 08:44:28 | INFO | Validation loss: 0.549, acc: 0.813, F1: 0.606
| 2021-07-13 08:44:28 | INFO | Start epoch 10:
| 2021-07-13 08:44:28 | INFO | Train Loss: 0.090, tp: 13, fn: 2, fp: 0, tn: 49, Acc: 0.969, Prec: 1.000, Rec: 0.867, F1: 0.954
| 2021-07-13 08:45:33 | INFO | Validation tp: 43, fn: 99, fp: 88, tn: 771
| 2021-07-13 08:45:33 | INFO | Validation loss: 0.716, acc: 0.813, F1: 0.603
| 2021-07-13 08:45:33 | INFO | Start epoch 11:
| 2021-07-13 08:45:33 | INFO | Train Loss: 0.073, tp: 10, fn: 0, fp: 1, tn: 53, Acc: 0.984, Prec: 0.909, Rec: 1.000, F1: 0.972
| 2021-07-13 08:46:37 | INFO | Validation tp: 41, fn: 101, fp: 69, tn: 790
| 2021-07-13 08:46:37 | INFO | Validation loss: 0.745, acc: 0.830, F1: 0.614
| 2021-07-13 08:46:37 | INFO | Start epoch 12:
| 2021-07-13 08:46:38 | INFO | Train Loss: 0.018, tp: 12, fn: 0, fp: 0, tn: 52, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 08:47:42 | INFO | Validation tp: 34, fn: 108, fp: 63, tn: 796
| 2021-07-13 08:47:42 | INFO | Validation loss: 0.808, acc: 0.829, F1: 0.594
| 2021-07-13 08:47:42 | INFO | Start epoch 13:
| 2021-07-13 08:47:42 | INFO | Train Loss: 0.132, tp: 9, fn: 2, fp: 2, tn: 51, Acc: 0.938, Prec: 0.818, Rec: 0.818, F1: 0.890
| 2021-07-13 08:48:46 | INFO | Validation tp: 24, fn: 118, fp: 51, tn: 808
| 2021-07-13 08:48:46 | INFO | Validation loss: 0.876, acc: 0.831, F1: 0.563
| 2021-07-13 08:48:46 | INFO | Start epoch 14:
| 2021-07-13 08:48:47 | INFO | Train Loss: 0.004, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 08:49:51 | INFO | Validation tp: 32, fn: 110, fp: 55, tn: 804
| 2021-07-13 08:49:51 | INFO | Validation loss: 0.886, acc: 0.835, F1: 0.593
| 2021-07-13 08:49:51 | INFO | Start epoch 15:
| 2021-07-13 08:49:52 | INFO | Train Loss: 0.003, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 08:50:56 | INFO | Validation tp: 33, fn: 109, fp: 60, tn: 799
| 2021-07-13 08:50:56 | INFO | Validation loss: 0.891, acc: 0.831, F1: 0.593
| 2021-07-13 08:50:58 | INFO | 
==============================Start training==============================
| 2021-07-13 08:50:58 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 08:50:58 | INFO | 
lr: 2e-05

| 2021-07-13 08:51:14 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 08:51:14 | INFO | Start epoch 1:
| 2021-07-13 08:51:15 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 08:52:19 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:52:19 | INFO | Validation loss: 0.398, acc: 0.858, F1: 0.462
| 2021-07-13 08:52:19 | INFO | Start epoch 2:
| 2021-07-13 08:52:19 | INFO | Train Loss: 0.438, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 08:53:23 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:53:23 | INFO | Validation loss: 0.396, acc: 0.858, F1: 0.462
| 2021-07-13 08:53:23 | INFO | Start epoch 3:
| 2021-07-13 08:53:24 | INFO | Train Loss: 0.418, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 08:54:28 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:54:28 | INFO | Validation loss: 0.385, acc: 0.858, F1: 0.462
| 2021-07-13 08:54:28 | INFO | Start epoch 4:
| 2021-07-13 08:54:29 | INFO | Train Loss: 0.557, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 08:55:33 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:55:33 | INFO | Validation loss: 0.370, acc: 0.858, F1: 0.462
| 2021-07-13 08:55:33 | INFO | Start epoch 5:
| 2021-07-13 08:55:34 | INFO | Train Loss: 0.391, tp: 1, fn: 9, fp: 0, tn: 54, Acc: 0.859, Prec: 1.000, Rec: 0.100, F1: 0.552
| 2021-07-13 08:56:38 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:56:38 | INFO | Validation loss: 0.392, acc: 0.858, F1: 0.462
| 2021-07-13 08:56:38 | INFO | Start epoch 6:
| 2021-07-13 08:56:38 | INFO | Train Loss: 0.460, tp: 0, fn: 13, fp: 1, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 08:57:42 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 08:57:42 | INFO | Validation loss: 0.415, acc: 0.858, F1: 0.462
| 2021-07-13 08:57:42 | INFO | Start epoch 7:
| 2021-07-13 08:57:43 | INFO | Train Loss: 0.521, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 08:58:47 | INFO | Validation tp: 2, fn: 140, fp: 4, tn: 855
| 2021-07-13 08:58:47 | INFO | Validation loss: 0.398, acc: 0.856, F1: 0.475
| 2021-07-13 08:58:47 | INFO | Start epoch 8:
| 2021-07-13 08:58:47 | INFO | Train Loss: 0.420, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 08:59:51 | INFO | Validation tp: 19, fn: 123, fp: 35, tn: 824
| 2021-07-13 08:59:51 | INFO | Validation loss: 0.401, acc: 0.842, F1: 0.553
| 2021-07-13 08:59:51 | INFO | Start epoch 9:
| 2021-07-13 08:59:52 | INFO | Train Loss: 0.287, tp: 0, fn: 6, fp: 1, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-13 09:00:56 | INFO | Validation tp: 10, fn: 132, fp: 20, tn: 839
| 2021-07-13 09:00:56 | INFO | Validation loss: 0.416, acc: 0.848, F1: 0.517
| 2021-07-13 09:00:56 | INFO | Start epoch 10:
| 2021-07-13 09:00:56 | INFO | Train Loss: 0.423, tp: 3, fn: 12, fp: 0, tn: 49, Acc: 0.812, Prec: 1.000, Rec: 0.200, F1: 0.612
| 2021-07-13 09:02:00 | INFO | Validation tp: 21, fn: 121, fp: 26, tn: 833
| 2021-07-13 09:02:00 | INFO | Validation loss: 0.458, acc: 0.853, F1: 0.571
| 2021-07-13 09:02:00 | INFO | Start epoch 11:
| 2021-07-13 09:02:01 | INFO | Train Loss: 0.216, tp: 4, fn: 6, fp: 0, tn: 54, Acc: 0.906, Prec: 1.000, Rec: 0.400, F1: 0.759
| 2021-07-13 09:03:05 | INFO | Validation tp: 36, fn: 106, fp: 69, tn: 790
| 2021-07-13 09:03:05 | INFO | Validation loss: 0.488, acc: 0.825, F1: 0.596
| 2021-07-13 09:03:05 | INFO | Start epoch 12:
| 2021-07-13 09:03:05 | INFO | Train Loss: 0.287, tp: 7, fn: 5, fp: 2, tn: 50, Acc: 0.891, Prec: 0.778, Rec: 0.583, F1: 0.801
| 2021-07-13 09:04:10 | INFO | Validation tp: 23, fn: 119, fp: 36, tn: 823
| 2021-07-13 09:04:10 | INFO | Validation loss: 0.499, acc: 0.845, F1: 0.571
| 2021-07-13 09:04:10 | INFO | Start epoch 13:
| 2021-07-13 09:04:10 | INFO | Train Loss: 0.242, tp: 7, fn: 4, fp: 0, tn: 53, Acc: 0.938, Prec: 1.000, Rec: 0.636, F1: 0.871
| 2021-07-13 09:05:14 | INFO | Validation tp: 31, fn: 111, fp: 56, tn: 803
| 2021-07-13 09:05:14 | INFO | Validation loss: 0.541, acc: 0.833, F1: 0.588
| 2021-07-13 09:05:14 | INFO | Start epoch 14:
| 2021-07-13 09:05:15 | INFO | Train Loss: 0.356, tp: 9, fn: 10, fp: 0, tn: 45, Acc: 0.844, Prec: 1.000, Rec: 0.474, F1: 0.771
| 2021-07-13 09:06:19 | INFO | Validation tp: 29, fn: 113, fp: 50, tn: 809
| 2021-07-13 09:06:19 | INFO | Validation loss: 0.556, acc: 0.837, F1: 0.585
| 2021-07-13 09:06:19 | INFO | Start epoch 15:
| 2021-07-13 09:06:19 | INFO | Train Loss: 0.126, tp: 9, fn: 1, fp: 1, tn: 53, Acc: 0.969, Prec: 0.900, Rec: 0.900, F1: 0.941
| 2021-07-13 09:07:23 | INFO | Validation tp: 26, fn: 116, fp: 45, tn: 814
| 2021-07-13 09:07:23 | INFO | Validation loss: 0.565, acc: 0.839, F1: 0.577
| 2021-07-13 09:07:26 | INFO | 
==============================Start training==============================
| 2021-07-13 09:07:26 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 09:07:26 | INFO | 
lr: 3e-05

| 2021-07-13 09:07:43 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 09:07:43 | INFO | Start epoch 1:
| 2021-07-13 09:07:43 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 09:08:47 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:08:47 | INFO | Validation loss: 0.399, acc: 0.858, F1: 0.462
| 2021-07-13 09:08:47 | INFO | Start epoch 2:
| 2021-07-13 09:08:48 | INFO | Train Loss: 0.409, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 09:09:52 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:09:52 | INFO | Validation loss: 0.402, acc: 0.858, F1: 0.462
| 2021-07-13 09:09:52 | INFO | Start epoch 3:
| 2021-07-13 09:09:53 | INFO | Train Loss: 0.419, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 09:10:57 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:10:57 | INFO | Validation loss: 0.393, acc: 0.858, F1: 0.462
| 2021-07-13 09:10:57 | INFO | Start epoch 4:
| 2021-07-13 09:10:57 | INFO | Train Loss: 0.527, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 09:12:01 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:12:01 | INFO | Validation loss: 0.383, acc: 0.858, F1: 0.462
| 2021-07-13 09:12:01 | INFO | Start epoch 5:
| 2021-07-13 09:12:02 | INFO | Train Loss: 0.394, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 09:13:06 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:13:06 | INFO | Validation loss: 0.379, acc: 0.858, F1: 0.462
| 2021-07-13 09:13:06 | INFO | Start epoch 6:
| 2021-07-13 09:13:07 | INFO | Train Loss: 0.360, tp: 2, fn: 11, fp: 0, tn: 51, Acc: 0.828, Prec: 1.000, Rec: 0.154, F1: 0.585
| 2021-07-13 09:14:11 | INFO | Validation tp: 8, fn: 134, fp: 10, tn: 849
| 2021-07-13 09:14:11 | INFO | Validation loss: 0.416, acc: 0.856, F1: 0.511
| 2021-07-13 09:14:11 | INFO | Start epoch 7:
| 2021-07-13 09:14:11 | INFO | Train Loss: 0.449, tp: 1, fn: 13, fp: 1, tn: 49, Acc: 0.781, Prec: 0.500, Rec: 0.071, F1: 0.500
| 2021-07-13 09:15:15 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:15:15 | INFO | Validation loss: 0.427, acc: 0.858, F1: 0.462
| 2021-07-13 09:15:15 | INFO | Start epoch 8:
| 2021-07-13 09:15:16 | INFO | Train Loss: 0.474, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 09:16:20 | INFO | Validation tp: 16, fn: 126, fp: 26, tn: 833
| 2021-07-13 09:16:20 | INFO | Validation loss: 0.429, acc: 0.848, F1: 0.545
| 2021-07-13 09:16:20 | INFO | Start epoch 9:
| 2021-07-13 09:16:20 | INFO | Train Loss: 0.227, tp: 1, fn: 5, fp: 1, tn: 57, Acc: 0.906, Prec: 0.500, Rec: 0.167, F1: 0.600
| 2021-07-13 09:17:25 | INFO | Validation tp: 33, fn: 109, fp: 68, tn: 791
| 2021-07-13 09:17:25 | INFO | Validation loss: 0.426, acc: 0.823, F1: 0.585
| 2021-07-13 09:17:25 | INFO | Start epoch 10:
| 2021-07-13 09:17:25 | INFO | Train Loss: 0.367, tp: 6, fn: 9, fp: 1, tn: 48, Acc: 0.844, Prec: 0.857, Rec: 0.400, F1: 0.726
| 2021-07-13 09:18:30 | INFO | Validation tp: 34, fn: 108, fp: 55, tn: 804
| 2021-07-13 09:18:30 | INFO | Validation loss: 0.461, acc: 0.837, F1: 0.601
| 2021-07-13 09:18:30 | INFO | Start epoch 11:
| 2021-07-13 09:18:30 | INFO | Train Loss: 0.254, tp: 6, fn: 4, fp: 4, tn: 50, Acc: 0.875, Prec: 0.600, Rec: 0.600, F1: 0.763
| 2021-07-13 09:19:35 | INFO | Validation tp: 38, fn: 104, fp: 63, tn: 796
| 2021-07-13 09:19:35 | INFO | Validation loss: 0.530, acc: 0.833, F1: 0.609
| 2021-07-13 09:19:35 | INFO | Start epoch 12:
| 2021-07-13 09:19:35 | INFO | Train Loss: 0.112, tp: 11, fn: 1, fp: 1, tn: 51, Acc: 0.969, Prec: 0.917, Rec: 0.917, F1: 0.949
| 2021-07-13 09:20:39 | INFO | Validation tp: 28, fn: 114, fp: 45, tn: 814
| 2021-07-13 09:20:39 | INFO | Validation loss: 0.569, acc: 0.841, F1: 0.586
| 2021-07-13 09:20:39 | INFO | Start epoch 13:
| 2021-07-13 09:20:40 | INFO | Train Loss: 0.194, tp: 10, fn: 1, fp: 2, tn: 51, Acc: 0.953, Prec: 0.833, Rec: 0.909, F1: 0.920
| 2021-07-13 09:21:44 | INFO | Validation tp: 34, fn: 108, fp: 57, tn: 802
| 2021-07-13 09:21:44 | INFO | Validation loss: 0.649, acc: 0.835, F1: 0.599
| 2021-07-13 09:21:44 | INFO | Start epoch 14:
| 2021-07-13 09:21:45 | INFO | Train Loss: 0.258, tp: 16, fn: 3, fp: 2, tn: 43, Acc: 0.922, Prec: 0.889, Rec: 0.842, F1: 0.905
| 2021-07-13 09:22:49 | INFO | Validation tp: 27, fn: 115, fp: 40, tn: 819
| 2021-07-13 09:22:49 | INFO | Validation loss: 0.646, acc: 0.845, F1: 0.586
| 2021-07-13 09:22:49 | INFO | Start epoch 15:
| 2021-07-13 09:22:50 | INFO | Train Loss: 0.071, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-13 09:23:54 | INFO | Validation tp: 29, fn: 113, fp: 40, tn: 819
| 2021-07-13 09:23:54 | INFO | Validation loss: 0.660, acc: 0.847, F1: 0.595
| 2021-07-13 09:23:56 | INFO | 
==============================Start training==============================
| 2021-07-13 09:23:56 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 09:23:56 | INFO | 
lr: 4e-05

| 2021-07-13 09:24:14 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 09:24:14 | INFO | Start epoch 1:
| 2021-07-13 09:24:14 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 09:25:18 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:25:18 | INFO | Validation loss: 0.407, acc: 0.858, F1: 0.462
| 2021-07-13 09:25:18 | INFO | Start epoch 2:
| 2021-07-13 09:25:19 | INFO | Train Loss: 0.386, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 09:26:23 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:26:23 | INFO | Validation loss: 0.389, acc: 0.858, F1: 0.462
| 2021-07-13 09:26:23 | INFO | Start epoch 3:
| 2021-07-13 09:26:24 | INFO | Train Loss: 0.427, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 09:27:28 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:27:28 | INFO | Validation loss: 0.381, acc: 0.858, F1: 0.462
| 2021-07-13 09:27:28 | INFO | Start epoch 4:
| 2021-07-13 09:27:28 | INFO | Train Loss: 0.519, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 09:28:32 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:28:32 | INFO | Validation loss: 0.374, acc: 0.858, F1: 0.462
| 2021-07-13 09:28:32 | INFO | Start epoch 5:
| 2021-07-13 09:28:33 | INFO | Train Loss: 0.361, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 09:29:37 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:29:37 | INFO | Validation loss: 0.382, acc: 0.858, F1: 0.462
| 2021-07-13 09:29:37 | INFO | Start epoch 6:
| 2021-07-13 09:29:38 | INFO | Train Loss: 0.325, tp: 1, fn: 12, fp: 0, tn: 51, Acc: 0.812, Prec: 1.000, Rec: 0.077, F1: 0.519
| 2021-07-13 09:30:41 | INFO | Validation tp: 13, fn: 129, fp: 10, tn: 849
| 2021-07-13 09:30:41 | INFO | Validation loss: 0.417, acc: 0.861, F1: 0.541
| 2021-07-13 09:30:41 | INFO | Start epoch 7:
| 2021-07-13 09:30:42 | INFO | Train Loss: 0.362, tp: 4, fn: 10, fp: 1, tn: 49, Acc: 0.828, Prec: 0.800, Rec: 0.286, F1: 0.660
| 2021-07-13 09:31:45 | INFO | Validation tp: 7, fn: 135, fp: 6, tn: 853
| 2021-07-13 09:31:45 | INFO | Validation loss: 0.487, acc: 0.859, F1: 0.507
| 2021-07-13 09:31:45 | INFO | Start epoch 8:
| 2021-07-13 09:31:46 | INFO | Train Loss: 0.253, tp: 2, fn: 9, fp: 0, tn: 53, Acc: 0.859, Prec: 1.000, Rec: 0.182, F1: 0.615
| 2021-07-13 09:32:49 | INFO | Validation tp: 26, fn: 116, fp: 70, tn: 789
| 2021-07-13 09:32:49 | INFO | Validation loss: 0.511, acc: 0.814, F1: 0.557
| 2021-07-13 09:32:49 | INFO | Start epoch 9:
| 2021-07-13 09:32:50 | INFO | Train Loss: 0.092, tp: 4, fn: 2, fp: 0, tn: 58, Acc: 0.969, Prec: 1.000, Rec: 0.667, F1: 0.892
| 2021-07-13 09:33:53 | INFO | Validation tp: 30, fn: 112, fp: 60, tn: 799
| 2021-07-13 09:33:53 | INFO | Validation loss: 0.563, acc: 0.828, F1: 0.581
| 2021-07-13 09:33:53 | INFO | Start epoch 10:
| 2021-07-13 09:33:54 | INFO | Train Loss: 0.157, tp: 14, fn: 1, fp: 1, tn: 48, Acc: 0.969, Prec: 0.933, Rec: 0.933, F1: 0.956
| 2021-07-13 09:34:57 | INFO | Validation tp: 26, fn: 116, fp: 55, tn: 804
| 2021-07-13 09:34:57 | INFO | Validation loss: 0.667, acc: 0.829, F1: 0.569
| 2021-07-13 09:34:57 | INFO | Start epoch 11:
| 2021-07-13 09:34:58 | INFO | Train Loss: 0.136, tp: 9, fn: 1, fp: 2, tn: 52, Acc: 0.953, Prec: 0.818, Rec: 0.900, F1: 0.915
| 2021-07-13 09:36:02 | INFO | Validation tp: 35, fn: 107, fp: 76, tn: 783
| 2021-07-13 09:36:02 | INFO | Validation loss: 0.704, acc: 0.817, F1: 0.586
| 2021-07-13 09:36:02 | INFO | Start epoch 12:
| 2021-07-13 09:36:03 | INFO | Train Loss: 0.093, tp: 12, fn: 0, fp: 1, tn: 51, Acc: 0.984, Prec: 0.923, Rec: 1.000, F1: 0.975
| 2021-07-13 09:37:07 | INFO | Validation tp: 15, fn: 127, fp: 34, tn: 825
| 2021-07-13 09:37:07 | INFO | Validation loss: 0.747, acc: 0.839, F1: 0.534
| 2021-07-13 09:37:07 | INFO | Start epoch 13:
| 2021-07-13 09:37:07 | INFO | Train Loss: 0.091, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-13 09:38:11 | INFO | Validation tp: 32, fn: 110, fp: 65, tn: 794
| 2021-07-13 09:38:11 | INFO | Validation loss: 0.829, acc: 0.825, F1: 0.584
| 2021-07-13 09:38:11 | INFO | Start epoch 14:
| 2021-07-13 09:38:12 | INFO | Train Loss: 0.006, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 09:39:16 | INFO | Validation tp: 27, fn: 115, fp: 47, tn: 812
| 2021-07-13 09:39:16 | INFO | Validation loss: 0.846, acc: 0.838, F1: 0.580
| 2021-07-13 09:39:16 | INFO | Start epoch 15:
| 2021-07-13 09:39:17 | INFO | Train Loss: 0.005, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 09:40:21 | INFO | Validation tp: 23, fn: 119, fp: 46, tn: 813
| 2021-07-13 09:40:21 | INFO | Validation loss: 0.845, acc: 0.835, F1: 0.563
| 2021-07-13 09:40:23 | INFO | 
==============================Start training==============================
| 2021-07-13 09:40:23 | INFO | Command Line Args:   --lr 5e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 09:40:23 | INFO | 
lr: 5e-05

| 2021-07-13 09:40:40 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 09:40:40 | INFO | Start epoch 1:
| 2021-07-13 09:40:41 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 09:41:45 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:41:45 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 09:41:45 | INFO | Start epoch 2:
| 2021-07-13 09:41:45 | INFO | Train Loss: 0.433, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 09:42:49 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:42:49 | INFO | Validation loss: 0.392, acc: 0.858, F1: 0.462
| 2021-07-13 09:42:49 | INFO | Start epoch 3:
| 2021-07-13 09:42:50 | INFO | Train Loss: 0.415, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 09:43:53 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:43:53 | INFO | Validation loss: 0.379, acc: 0.858, F1: 0.462
| 2021-07-13 09:43:53 | INFO | Start epoch 4:
| 2021-07-13 09:43:54 | INFO | Train Loss: 0.518, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 09:44:58 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:44:58 | INFO | Validation loss: 0.390, acc: 0.858, F1: 0.462
| 2021-07-13 09:44:58 | INFO | Start epoch 5:
| 2021-07-13 09:44:58 | INFO | Train Loss: 0.382, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 09:46:02 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:46:02 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 09:46:02 | INFO | Start epoch 6:
| 2021-07-13 09:46:03 | INFO | Train Loss: 0.528, tp: 0, fn: 13, fp: 0, tn: 51, Acc: 0.797, Prec: 0.000, Rec: 0.000, F1: 0.443
| 2021-07-13 09:47:07 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:47:07 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 09:47:07 | INFO | Start epoch 7:
| 2021-07-13 09:47:08 | INFO | Train Loss: 0.566, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 09:48:12 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:48:12 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 09:48:12 | INFO | Start epoch 8:
| 2021-07-13 09:48:13 | INFO | Train Loss: 0.474, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 09:49:17 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:49:17 | INFO | Validation loss: 0.414, acc: 0.858, F1: 0.462
| 2021-07-13 09:49:17 | INFO | Start epoch 9:
| 2021-07-13 09:49:18 | INFO | Train Loss: 0.321, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 09:50:21 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:50:21 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 09:50:21 | INFO | Start epoch 10:
| 2021-07-13 09:50:22 | INFO | Train Loss: 0.563, tp: 0, fn: 15, fp: 0, tn: 49, Acc: 0.766, Prec: 0.000, Rec: 0.000, F1: 0.434
| 2021-07-13 09:51:26 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:51:26 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 09:51:26 | INFO | Start epoch 11:
| 2021-07-13 09:51:26 | INFO | Train Loss: 0.442, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 09:52:31 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:52:31 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 09:52:31 | INFO | Start epoch 12:
| 2021-07-13 09:52:31 | INFO | Train Loss: 0.478, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-13 09:53:35 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:53:35 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 09:53:35 | INFO | Start epoch 13:
| 2021-07-13 09:53:36 | INFO | Train Loss: 0.471, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 09:54:39 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:54:39 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 09:54:39 | INFO | Start epoch 14:
| 2021-07-13 09:54:40 | INFO | Train Loss: 0.659, tp: 0, fn: 19, fp: 0, tn: 45, Acc: 0.703, Prec: 0.000, Rec: 0.000, F1: 0.413
| 2021-07-13 09:55:44 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:55:44 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 09:55:44 | INFO | Start epoch 15:
| 2021-07-13 09:55:45 | INFO | Train Loss: 0.430, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 09:56:49 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:56:49 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 09:56:51 | INFO | 
==============================Start training==============================
| 2021-07-13 09:56:51 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 09:56:51 | INFO | 
lr: 2e-05

| 2021-07-13 09:57:08 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 09:57:08 | INFO | Start epoch 1:
| 2021-07-13 09:57:09 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 09:58:13 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:58:13 | INFO | Validation loss: 0.404, acc: 0.858, F1: 0.462
| 2021-07-13 09:58:13 | INFO | Start epoch 2:
| 2021-07-13 09:58:14 | INFO | Train Loss: 0.423, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 09:59:18 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 09:59:18 | INFO | Validation loss: 0.403, acc: 0.858, F1: 0.462
| 2021-07-13 09:59:18 | INFO | Start epoch 3:
| 2021-07-13 09:59:18 | INFO | Train Loss: 0.436, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 10:00:22 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:00:22 | INFO | Validation loss: 0.406, acc: 0.858, F1: 0.462
| 2021-07-13 10:00:22 | INFO | Start epoch 4:
| 2021-07-13 10:00:23 | INFO | Train Loss: 0.571, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 10:01:27 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:01:27 | INFO | Validation loss: 0.376, acc: 0.858, F1: 0.462
| 2021-07-13 10:01:27 | INFO | Start epoch 5:
| 2021-07-13 10:01:27 | INFO | Train Loss: 0.386, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 10:02:31 | INFO | Validation tp: 27, fn: 115, fp: 29, tn: 830
| 2021-07-13 10:02:31 | INFO | Validation loss: 0.369, acc: 0.856, F1: 0.596
| 2021-07-13 10:02:31 | INFO | Start epoch 6:
| 2021-07-13 10:02:32 | INFO | Train Loss: 0.407, tp: 3, fn: 10, fp: 2, tn: 49, Acc: 0.812, Prec: 0.600, Rec: 0.231, F1: 0.612
| 2021-07-13 10:03:36 | INFO | Validation tp: 2, fn: 140, fp: 1, tn: 858
| 2021-07-13 10:03:36 | INFO | Validation loss: 0.402, acc: 0.859, F1: 0.476
| 2021-07-13 10:03:36 | INFO | Start epoch 7:
| 2021-07-13 10:03:36 | INFO | Train Loss: 0.472, tp: 1, fn: 13, fp: 1, tn: 49, Acc: 0.781, Prec: 0.500, Rec: 0.071, F1: 0.500
| 2021-07-13 10:04:40 | INFO | Validation tp: 8, fn: 134, fp: 10, tn: 849
| 2021-07-13 10:04:40 | INFO | Validation loss: 0.398, acc: 0.856, F1: 0.511
| 2021-07-13 10:04:40 | INFO | Start epoch 8:
| 2021-07-13 10:04:41 | INFO | Train Loss: 0.291, tp: 1, fn: 10, fp: 0, tn: 53, Acc: 0.844, Prec: 1.000, Rec: 0.091, F1: 0.540
| 2021-07-13 10:05:45 | INFO | Validation tp: 37, fn: 105, fp: 59, tn: 800
| 2021-07-13 10:05:45 | INFO | Validation loss: 0.417, acc: 0.836, F1: 0.609
| 2021-07-13 10:05:45 | INFO | Start epoch 9:
| 2021-07-13 10:05:46 | INFO | Train Loss: 0.224, tp: 4, fn: 2, fp: 2, tn: 56, Acc: 0.938, Prec: 0.667, Rec: 0.667, F1: 0.816
| 2021-07-13 10:06:50 | INFO | Validation tp: 42, fn: 100, fp: 66, tn: 793
| 2021-07-13 10:06:50 | INFO | Validation loss: 0.471, acc: 0.834, F1: 0.621
| 2021-07-13 10:06:50 | INFO | Start epoch 10:
| 2021-07-13 10:06:50 | INFO | Train Loss: 0.141, tp: 13, fn: 2, fp: 1, tn: 48, Acc: 0.953, Prec: 0.929, Rec: 0.867, F1: 0.933
| 2021-07-13 10:07:54 | INFO | Validation tp: 30, fn: 112, fp: 48, tn: 811
| 2021-07-13 10:07:54 | INFO | Validation loss: 0.538, acc: 0.840, F1: 0.591
| 2021-07-13 10:07:54 | INFO | Start epoch 11:
| 2021-07-13 10:07:55 | INFO | Train Loss: 0.093, tp: 8, fn: 2, fp: 1, tn: 53, Acc: 0.953, Prec: 0.889, Rec: 0.800, F1: 0.907
| 2021-07-13 10:08:59 | INFO | Validation tp: 61, fn: 81, fp: 111, tn: 748
| 2021-07-13 10:08:59 | INFO | Validation loss: 0.642, acc: 0.808, F1: 0.637
| 2021-07-13 10:08:59 | INFO | Start epoch 12:
| 2021-07-13 10:08:59 | INFO | Train Loss: 0.098, tp: 12, fn: 0, fp: 3, tn: 49, Acc: 0.953, Prec: 0.800, Rec: 1.000, F1: 0.930
| 2021-07-13 10:10:03 | INFO | Validation tp: 30, fn: 112, fp: 74, tn: 785
| 2021-07-13 10:10:03 | INFO | Validation loss: 0.680, acc: 0.814, F1: 0.569
| 2021-07-13 10:10:03 | INFO | Start epoch 13:
| 2021-07-13 10:10:04 | INFO | Train Loss: 0.096, tp: 11, fn: 0, fp: 1, tn: 52, Acc: 0.984, Prec: 0.917, Rec: 1.000, F1: 0.973
| 2021-07-13 10:11:08 | INFO | Validation tp: 31, fn: 111, fp: 65, tn: 794
| 2021-07-13 10:11:08 | INFO | Validation loss: 0.831, acc: 0.824, F1: 0.580
| 2021-07-13 10:11:08 | INFO | Start epoch 14:
| 2021-07-13 10:11:08 | INFO | Train Loss: 0.006, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:12:13 | INFO | Validation tp: 30, fn: 112, fp: 51, tn: 808
| 2021-07-13 10:12:13 | INFO | Validation loss: 0.857, acc: 0.837, F1: 0.589
| 2021-07-13 10:12:13 | INFO | Start epoch 15:
| 2021-07-13 10:12:13 | INFO | Train Loss: 0.043, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 10:13:17 | INFO | Validation tp: 29, fn: 113, fp: 59, tn: 800
| 2021-07-13 10:13:17 | INFO | Validation loss: 0.904, acc: 0.828, F1: 0.578
| 2021-07-13 10:13:17 | INFO | Start epoch 16:
| 2021-07-13 10:13:18 | INFO | Train Loss: 0.005, tp: 13, fn: 0, fp: 0, tn: 51, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:14:22 | INFO | Validation tp: 26, fn: 116, fp: 39, tn: 820
| 2021-07-13 10:14:22 | INFO | Validation loss: 0.912, acc: 0.845, F1: 0.582
| 2021-07-13 10:14:22 | INFO | Start epoch 17:
| 2021-07-13 10:14:22 | INFO | Train Loss: 0.005, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:15:27 | INFO | Validation tp: 24, fn: 118, fp: 47, tn: 812
| 2021-07-13 10:15:27 | INFO | Validation loss: 0.971, acc: 0.835, F1: 0.567
| 2021-07-13 10:15:27 | INFO | Start epoch 18:
| 2021-07-13 10:15:27 | INFO | Train Loss: 0.003, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:16:32 | INFO | Validation tp: 25, fn: 117, fp: 53, tn: 806
| 2021-07-13 10:16:32 | INFO | Validation loss: 1.016, acc: 0.830, F1: 0.566
| 2021-07-13 10:16:32 | INFO | Start epoch 19:
| 2021-07-13 10:16:32 | INFO | Train Loss: 0.003, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:17:36 | INFO | Validation tp: 23, fn: 119, fp: 49, tn: 810
| 2021-07-13 10:17:36 | INFO | Validation loss: 1.024, acc: 0.832, F1: 0.560
| 2021-07-13 10:17:36 | INFO | Start epoch 20:
| 2021-07-13 10:17:37 | INFO | Train Loss: 0.004, tp: 12, fn: 0, fp: 0, tn: 52, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:18:41 | INFO | Validation tp: 28, fn: 114, fp: 58, tn: 801
| 2021-07-13 10:18:41 | INFO | Validation loss: 1.019, acc: 0.828, F1: 0.574
| 2021-07-13 10:18:43 | INFO | 
==============================Start training==============================
| 2021-07-13 10:18:43 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 10:18:43 | INFO | 
lr: 3e-05

| 2021-07-13 10:19:00 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 10:19:00 | INFO | Start epoch 1:
| 2021-07-13 10:19:00 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 10:20:04 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:20:04 | INFO | Validation loss: 0.406, acc: 0.858, F1: 0.462
| 2021-07-13 10:20:04 | INFO | Start epoch 2:
| 2021-07-13 10:20:05 | INFO | Train Loss: 0.414, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 10:21:09 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:21:09 | INFO | Validation loss: 0.402, acc: 0.858, F1: 0.462
| 2021-07-13 10:21:09 | INFO | Start epoch 3:
| 2021-07-13 10:21:10 | INFO | Train Loss: 0.426, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 10:22:14 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:22:14 | INFO | Validation loss: 0.388, acc: 0.858, F1: 0.462
| 2021-07-13 10:22:14 | INFO | Start epoch 4:
| 2021-07-13 10:22:14 | INFO | Train Loss: 0.537, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 10:23:18 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:23:18 | INFO | Validation loss: 0.377, acc: 0.858, F1: 0.462
| 2021-07-13 10:23:18 | INFO | Start epoch 5:
| 2021-07-13 10:23:19 | INFO | Train Loss: 0.397, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 10:24:23 | INFO | Validation tp: 15, fn: 127, fp: 17, tn: 842
| 2021-07-13 10:24:23 | INFO | Validation loss: 0.380, acc: 0.856, F1: 0.547
| 2021-07-13 10:24:23 | INFO | Start epoch 6:
| 2021-07-13 10:24:23 | INFO | Train Loss: 0.339, tp: 3, fn: 10, fp: 2, tn: 49, Acc: 0.812, Prec: 0.600, Rec: 0.231, F1: 0.612
| 2021-07-13 10:25:28 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:25:28 | INFO | Validation loss: 0.418, acc: 0.858, F1: 0.462
| 2021-07-13 10:25:28 | INFO | Start epoch 7:
| 2021-07-13 10:25:28 | INFO | Train Loss: 0.515, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 10:26:32 | INFO | Validation tp: 14, fn: 128, fp: 11, tn: 848
| 2021-07-13 10:26:32 | INFO | Validation loss: 0.436, acc: 0.861, F1: 0.546
| 2021-07-13 10:26:32 | INFO | Start epoch 8:
| 2021-07-13 10:26:33 | INFO | Train Loss: 0.318, tp: 3, fn: 8, fp: 0, tn: 53, Acc: 0.875, Prec: 1.000, Rec: 0.273, F1: 0.679
| 2021-07-13 10:27:37 | INFO | Validation tp: 14, fn: 128, fp: 30, tn: 829
| 2021-07-13 10:27:37 | INFO | Validation loss: 0.477, acc: 0.842, F1: 0.532
| 2021-07-13 10:27:37 | INFO | Start epoch 9:
| 2021-07-13 10:27:38 | INFO | Train Loss: 0.200, tp: 2, fn: 4, fp: 1, tn: 57, Acc: 0.922, Prec: 0.667, Rec: 0.333, F1: 0.701
| 2021-07-13 10:28:42 | INFO | Validation tp: 32, fn: 110, fp: 46, tn: 813
| 2021-07-13 10:28:42 | INFO | Validation loss: 0.523, acc: 0.844, F1: 0.602
| 2021-07-13 10:28:42 | INFO | Start epoch 10:
| 2021-07-13 10:28:42 | INFO | Train Loss: 0.141, tp: 13, fn: 2, fp: 1, tn: 48, Acc: 0.953, Prec: 0.929, Rec: 0.867, F1: 0.933
| 2021-07-13 10:29:47 | INFO | Validation tp: 33, fn: 109, fp: 55, tn: 804
| 2021-07-13 10:29:47 | INFO | Validation loss: 0.584, acc: 0.836, F1: 0.597
| 2021-07-13 10:29:47 | INFO | Start epoch 11:
| 2021-07-13 10:29:47 | INFO | Train Loss: 0.049, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:30:51 | INFO | Validation tp: 43, fn: 99, fp: 76, tn: 783
| 2021-07-13 10:30:51 | INFO | Validation loss: 0.621, acc: 0.825, F1: 0.614
| 2021-07-13 10:30:51 | INFO | Start epoch 12:
| 2021-07-13 10:30:52 | INFO | Train Loss: 0.087, tp: 11, fn: 1, fp: 1, tn: 51, Acc: 0.969, Prec: 0.917, Rec: 0.917, F1: 0.949
| 2021-07-13 10:31:56 | INFO | Validation tp: 38, fn: 104, fp: 72, tn: 787
| 2021-07-13 10:31:56 | INFO | Validation loss: 0.651, acc: 0.824, F1: 0.601
| 2021-07-13 10:31:56 | INFO | Start epoch 13:
| 2021-07-13 10:31:56 | INFO | Train Loss: 0.093, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-13 10:33:00 | INFO | Validation tp: 37, fn: 105, fp: 75, tn: 784
| 2021-07-13 10:33:00 | INFO | Validation loss: 0.823, acc: 0.820, F1: 0.594
| 2021-07-13 10:33:00 | INFO | Start epoch 14:
| 2021-07-13 10:33:01 | INFO | Train Loss: 0.022, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:34:05 | INFO | Validation tp: 25, fn: 117, fp: 34, tn: 825
| 2021-07-13 10:34:05 | INFO | Validation loss: 0.709, acc: 0.849, F1: 0.582
| 2021-07-13 10:34:05 | INFO | Start epoch 15:
| 2021-07-13 10:34:05 | INFO | Train Loss: 0.011, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:35:09 | INFO | Validation tp: 37, fn: 105, fp: 62, tn: 797
| 2021-07-13 10:35:09 | INFO | Validation loss: 0.835, acc: 0.833, F1: 0.606
| 2021-07-13 10:35:09 | INFO | Start epoch 16:
| 2021-07-13 10:35:10 | INFO | Train Loss: 0.004, tp: 13, fn: 0, fp: 0, tn: 51, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:36:13 | INFO | Validation tp: 28, fn: 114, fp: 40, tn: 819
| 2021-07-13 10:36:13 | INFO | Validation loss: 0.918, acc: 0.846, F1: 0.590
| 2021-07-13 10:36:13 | INFO | Start epoch 17:
| 2021-07-13 10:36:14 | INFO | Train Loss: 0.008, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:37:18 | INFO | Validation tp: 39, fn: 103, fp: 63, tn: 796
| 2021-07-13 10:37:18 | INFO | Validation loss: 1.015, acc: 0.834, F1: 0.613
| 2021-07-13 10:37:18 | INFO | Start epoch 18:
| 2021-07-13 10:37:19 | INFO | Train Loss: 0.001, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:38:23 | INFO | Validation tp: 27, fn: 115, fp: 41, tn: 818
| 2021-07-13 10:38:23 | INFO | Validation loss: 1.009, acc: 0.844, F1: 0.585
| 2021-07-13 10:38:23 | INFO | Start epoch 19:
| 2021-07-13 10:38:23 | INFO | Train Loss: 0.006, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:39:27 | INFO | Validation tp: 36, fn: 106, fp: 58, tn: 801
| 2021-07-13 10:39:27 | INFO | Validation loss: 1.018, acc: 0.836, F1: 0.606
| 2021-07-13 10:39:27 | INFO | Start epoch 20:
| 2021-07-13 10:39:28 | INFO | Train Loss: 0.001, tp: 12, fn: 0, fp: 0, tn: 52, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:40:32 | INFO | Validation tp: 34, fn: 108, fp: 51, tn: 808
| 2021-07-13 10:40:32 | INFO | Validation loss: 1.023, acc: 0.841, F1: 0.605
| 2021-07-13 10:40:35 | INFO | 
==============================Start training==============================
| 2021-07-13 10:40:35 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 10:40:35 | INFO | 
lr: 4e-05

| 2021-07-13 10:40:51 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 10:40:51 | INFO | Start epoch 1:
| 2021-07-13 10:40:52 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 10:41:56 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:41:56 | INFO | Validation loss: 0.399, acc: 0.858, F1: 0.462
| 2021-07-13 10:41:56 | INFO | Start epoch 2:
| 2021-07-13 10:41:56 | INFO | Train Loss: 0.409, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 10:43:00 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:43:00 | INFO | Validation loss: 0.402, acc: 0.858, F1: 0.462
| 2021-07-13 10:43:00 | INFO | Start epoch 3:
| 2021-07-13 10:43:01 | INFO | Train Loss: 0.419, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 10:44:05 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:44:05 | INFO | Validation loss: 0.393, acc: 0.858, F1: 0.462
| 2021-07-13 10:44:05 | INFO | Start epoch 4:
| 2021-07-13 10:44:06 | INFO | Train Loss: 0.527, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 10:45:10 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:45:10 | INFO | Validation loss: 0.383, acc: 0.858, F1: 0.462
| 2021-07-13 10:45:10 | INFO | Start epoch 5:
| 2021-07-13 10:45:10 | INFO | Train Loss: 0.394, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 10:46:14 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 10:46:14 | INFO | Validation loss: 0.377, acc: 0.858, F1: 0.462
| 2021-07-13 10:46:14 | INFO | Start epoch 6:
| 2021-07-13 10:46:15 | INFO | Train Loss: 0.377, tp: 0, fn: 13, fp: 0, tn: 51, Acc: 0.797, Prec: 0.000, Rec: 0.000, F1: 0.443
| 2021-07-13 10:47:19 | INFO | Validation tp: 9, fn: 133, fp: 10, tn: 849
| 2021-07-13 10:47:19 | INFO | Validation loss: 0.393, acc: 0.857, F1: 0.517
| 2021-07-13 10:47:19 | INFO | Start epoch 7:
| 2021-07-13 10:47:19 | INFO | Train Loss: 0.425, tp: 2, fn: 12, fp: 1, tn: 49, Acc: 0.797, Prec: 0.667, Rec: 0.143, F1: 0.559
| 2021-07-13 10:48:24 | INFO | Validation tp: 10, fn: 132, fp: 12, tn: 847
| 2021-07-13 10:48:24 | INFO | Validation loss: 0.419, acc: 0.856, F1: 0.522
| 2021-07-13 10:48:24 | INFO | Start epoch 8:
| 2021-07-13 10:48:24 | INFO | Train Loss: 0.252, tp: 3, fn: 8, fp: 0, tn: 53, Acc: 0.875, Prec: 1.000, Rec: 0.273, F1: 0.679
| 2021-07-13 10:49:28 | INFO | Validation tp: 21, fn: 121, fp: 45, tn: 814
| 2021-07-13 10:49:28 | INFO | Validation loss: 0.420, acc: 0.834, F1: 0.555
| 2021-07-13 10:49:28 | INFO | Start epoch 9:
| 2021-07-13 10:49:29 | INFO | Train Loss: 0.204, tp: 4, fn: 2, fp: 1, tn: 57, Acc: 0.953, Prec: 0.800, Rec: 0.667, F1: 0.851
| 2021-07-13 10:50:33 | INFO | Validation tp: 38, fn: 104, fp: 77, tn: 782
| 2021-07-13 10:50:33 | INFO | Validation loss: 0.534, acc: 0.819, F1: 0.596
| 2021-07-13 10:50:33 | INFO | Start epoch 10:
| 2021-07-13 10:50:33 | INFO | Train Loss: 0.145, tp: 13, fn: 2, fp: 1, tn: 48, Acc: 0.953, Prec: 0.929, Rec: 0.867, F1: 0.933
| 2021-07-13 10:51:38 | INFO | Validation tp: 27, fn: 115, fp: 48, tn: 811
| 2021-07-13 10:51:38 | INFO | Validation loss: 0.601, acc: 0.837, F1: 0.579
| 2021-07-13 10:51:38 | INFO | Start epoch 11:
| 2021-07-13 10:51:38 | INFO | Train Loss: 0.092, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-13 10:52:42 | INFO | Validation tp: 22, fn: 120, fp: 57, tn: 802
| 2021-07-13 10:52:42 | INFO | Validation loss: 0.695, acc: 0.823, F1: 0.550
| 2021-07-13 10:52:42 | INFO | Start epoch 12:
| 2021-07-13 10:52:43 | INFO | Train Loss: 0.038, tp: 12, fn: 0, fp: 1, tn: 51, Acc: 0.984, Prec: 0.923, Rec: 1.000, F1: 0.975
| 2021-07-13 10:53:47 | INFO | Validation tp: 33, fn: 109, fp: 57, tn: 802
| 2021-07-13 10:53:47 | INFO | Validation loss: 0.716, acc: 0.834, F1: 0.595
| 2021-07-13 10:53:47 | INFO | Start epoch 13:
| 2021-07-13 10:53:48 | INFO | Train Loss: 0.074, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-13 10:54:52 | INFO | Validation tp: 22, fn: 120, fp: 60, tn: 799
| 2021-07-13 10:54:52 | INFO | Validation loss: 0.902, acc: 0.820, F1: 0.548
| 2021-07-13 10:54:52 | INFO | Start epoch 14:
| 2021-07-13 10:54:53 | INFO | Train Loss: 0.118, tp: 17, fn: 2, fp: 0, tn: 45, Acc: 0.969, Prec: 1.000, Rec: 0.895, F1: 0.961
| 2021-07-13 10:55:56 | INFO | Validation tp: 22, fn: 120, fp: 56, tn: 803
| 2021-07-13 10:55:56 | INFO | Validation loss: 0.901, acc: 0.824, F1: 0.551
| 2021-07-13 10:55:56 | INFO | Start epoch 15:
| 2021-07-13 10:55:57 | INFO | Train Loss: 0.029, tp: 10, fn: 0, fp: 1, tn: 53, Acc: 0.984, Prec: 0.909, Rec: 1.000, F1: 0.972
| 2021-07-13 10:57:00 | INFO | Validation tp: 25, fn: 117, fp: 54, tn: 805
| 2021-07-13 10:57:00 | INFO | Validation loss: 0.965, acc: 0.829, F1: 0.565
| 2021-07-13 10:57:00 | INFO | Start epoch 16:
| 2021-07-13 10:57:01 | INFO | Train Loss: 0.004, tp: 13, fn: 0, fp: 0, tn: 51, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 10:58:05 | INFO | Validation tp: 15, fn: 127, fp: 45, tn: 814
| 2021-07-13 10:58:05 | INFO | Validation loss: 0.955, acc: 0.828, F1: 0.526
| 2021-07-13 10:58:05 | INFO | Start epoch 17:
| 2021-07-13 10:58:06 | INFO | Train Loss: 0.032, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-13 10:59:09 | INFO | Validation tp: 29, fn: 113, fp: 61, tn: 798
| 2021-07-13 10:59:09 | INFO | Validation loss: 1.015, acc: 0.826, F1: 0.576
| 2021-07-13 10:59:09 | INFO | Start epoch 18:
| 2021-07-13 10:59:10 | INFO | Train Loss: 0.002, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 11:00:14 | INFO | Validation tp: 27, fn: 115, fp: 63, tn: 796
| 2021-07-13 11:00:14 | INFO | Validation loss: 1.047, acc: 0.822, F1: 0.566
| 2021-07-13 11:00:14 | INFO | Start epoch 19:
| 2021-07-13 11:00:15 | INFO | Train Loss: 0.001, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 11:01:19 | INFO | Validation tp: 20, fn: 122, fp: 51, tn: 808
| 2021-07-13 11:01:19 | INFO | Validation loss: 1.063, acc: 0.827, F1: 0.546
| 2021-07-13 11:01:19 | INFO | Start epoch 20:
| 2021-07-13 11:01:19 | INFO | Train Loss: 0.001, tp: 12, fn: 0, fp: 0, tn: 52, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 11:02:23 | INFO | Validation tp: 27, fn: 115, fp: 59, tn: 800
| 2021-07-13 11:02:23 | INFO | Validation loss: 1.081, acc: 0.826, F1: 0.569
| 2021-07-13 11:02:26 | INFO | 
==============================Start training==============================
| 2021-07-13 11:02:26 | INFO | Command Line Args:   --lr 5e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256

| 2021-07-13 11:02:26 | INFO | 
lr: 5e-05

| 2021-07-13 11:02:42 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 11:02:42 | INFO | Start epoch 1:
| 2021-07-13 11:02:43 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 11:03:47 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:03:47 | INFO | Validation loss: 0.404, acc: 0.858, F1: 0.462
| 2021-07-13 11:03:47 | INFO | Start epoch 2:
| 2021-07-13 11:03:47 | INFO | Train Loss: 0.430, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 11:04:52 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:04:52 | INFO | Validation loss: 0.392, acc: 0.858, F1: 0.462
| 2021-07-13 11:04:52 | INFO | Start epoch 3:
| 2021-07-13 11:04:52 | INFO | Train Loss: 0.427, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 11:05:56 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:05:56 | INFO | Validation loss: 0.367, acc: 0.858, F1: 0.462
| 2021-07-13 11:05:56 | INFO | Start epoch 4:
| 2021-07-13 11:05:57 | INFO | Train Loss: 0.513, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 11:07:01 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:07:01 | INFO | Validation loss: 0.381, acc: 0.858, F1: 0.462
| 2021-07-13 11:07:01 | INFO | Start epoch 5:
| 2021-07-13 11:07:02 | INFO | Train Loss: 0.356, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 11:08:06 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:08:06 | INFO | Validation loss: 0.377, acc: 0.858, F1: 0.462
| 2021-07-13 11:08:06 | INFO | Start epoch 6:
| 2021-07-13 11:08:07 | INFO | Train Loss: 0.398, tp: 0, fn: 13, fp: 0, tn: 51, Acc: 0.797, Prec: 0.000, Rec: 0.000, F1: 0.443
| 2021-07-13 11:09:11 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:09:11 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 11:09:11 | INFO | Start epoch 7:
| 2021-07-13 11:09:11 | INFO | Train Loss: 0.526, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 11:10:16 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:10:16 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 11:10:16 | INFO | Start epoch 8:
| 2021-07-13 11:10:16 | INFO | Train Loss: 0.474, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 11:11:20 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:11:20 | INFO | Validation loss: 0.413, acc: 0.858, F1: 0.462
| 2021-07-13 11:11:20 | INFO | Start epoch 9:
| 2021-07-13 11:11:21 | INFO | Train Loss: 0.323, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 11:12:25 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:12:25 | INFO | Validation loss: 0.413, acc: 0.858, F1: 0.462
| 2021-07-13 11:12:25 | INFO | Start epoch 10:
| 2021-07-13 11:12:26 | INFO | Train Loss: 0.565, tp: 0, fn: 15, fp: 0, tn: 49, Acc: 0.766, Prec: 0.000, Rec: 0.000, F1: 0.434
| 2021-07-13 11:13:30 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:13:30 | INFO | Validation loss: 0.413, acc: 0.858, F1: 0.462
| 2021-07-13 11:13:30 | INFO | Start epoch 11:
| 2021-07-13 11:13:31 | INFO | Train Loss: 0.443, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 11:14:35 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:14:35 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 11:14:35 | INFO | Start epoch 12:
| 2021-07-13 11:14:36 | INFO | Train Loss: 0.477, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-13 11:15:40 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:15:40 | INFO | Validation loss: 0.409, acc: 0.858, F1: 0.462
| 2021-07-13 11:15:40 | INFO | Start epoch 13:
| 2021-07-13 11:15:40 | INFO | Train Loss: 0.473, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 11:16:44 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:16:44 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:16:44 | INFO | Start epoch 14:
| 2021-07-13 11:16:45 | INFO | Train Loss: 0.654, tp: 0, fn: 19, fp: 0, tn: 45, Acc: 0.703, Prec: 0.000, Rec: 0.000, F1: 0.413
| 2021-07-13 11:17:49 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:17:49 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 11:17:49 | INFO | Start epoch 15:
| 2021-07-13 11:17:49 | INFO | Train Loss: 0.430, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 11:18:53 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:18:53 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:18:53 | INFO | Start epoch 16:
| 2021-07-13 11:18:54 | INFO | Train Loss: 0.506, tp: 0, fn: 13, fp: 0, tn: 51, Acc: 0.797, Prec: 0.000, Rec: 0.000, F1: 0.443
| 2021-07-13 11:19:58 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:19:58 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:19:58 | INFO | Start epoch 17:
| 2021-07-13 11:19:59 | INFO | Train Loss: 0.444, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 11:21:03 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:21:03 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:21:03 | INFO | Start epoch 18:
| 2021-07-13 11:21:04 | INFO | Train Loss: 0.538, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 11:22:08 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:22:08 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:22:08 | INFO | Start epoch 19:
| 2021-07-13 11:22:09 | INFO | Train Loss: 0.550, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 11:23:13 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:23:13 | INFO | Validation loss: 0.411, acc: 0.858, F1: 0.462
| 2021-07-13 11:23:13 | INFO | Start epoch 20:
| 2021-07-13 11:23:14 | INFO | Train Loss: 0.474, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-13 11:24:18 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:24:18 | INFO | Validation loss: 0.410, acc: 0.858, F1: 0.462
| 2021-07-13 11:24:21 | INFO | 
==============================Start training==============================
| 2021-07-13 11:24:21 | INFO | Command Line Args:   --lr 2e-5 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.3
Defaults:
  --hidden_size:     256

| 2021-07-13 11:24:21 | INFO | 
lr: 2e-05

| 2021-07-13 11:24:37 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 11:24:37 | INFO | Start epoch 1:
| 2021-07-13 11:24:37 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 11:25:41 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:25:41 | INFO | Validation loss: 0.398, acc: 0.858, F1: 0.462
| 2021-07-13 11:25:41 | INFO | Start epoch 2:
| 2021-07-13 11:25:42 | INFO | Train Loss: 0.438, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 11:26:46 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:26:46 | INFO | Validation loss: 0.396, acc: 0.858, F1: 0.462
| 2021-07-13 11:26:46 | INFO | Start epoch 3:
| 2021-07-13 11:26:47 | INFO | Train Loss: 0.418, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 11:27:51 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:27:51 | INFO | Validation loss: 0.385, acc: 0.858, F1: 0.462
| 2021-07-13 11:27:51 | INFO | Start epoch 4:
| 2021-07-13 11:27:52 | INFO | Train Loss: 0.557, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 11:28:56 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:28:56 | INFO | Validation loss: 0.370, acc: 0.858, F1: 0.462
| 2021-07-13 11:28:56 | INFO | Start epoch 5:
| 2021-07-13 11:28:56 | INFO | Train Loss: 0.391, tp: 1, fn: 9, fp: 0, tn: 54, Acc: 0.859, Prec: 1.000, Rec: 0.100, F1: 0.552
| 2021-07-13 11:30:01 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:30:01 | INFO | Validation loss: 0.392, acc: 0.858, F1: 0.462
| 2021-07-13 11:30:01 | INFO | Start epoch 6:
| 2021-07-13 11:30:01 | INFO | Train Loss: 0.460, tp: 0, fn: 13, fp: 1, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 11:31:05 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:31:05 | INFO | Validation loss: 0.415, acc: 0.858, F1: 0.462
| 2021-07-13 11:31:05 | INFO | Start epoch 7:
| 2021-07-13 11:31:06 | INFO | Train Loss: 0.521, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-13 11:32:10 | INFO | Validation tp: 2, fn: 140, fp: 4, tn: 855
| 2021-07-13 11:32:10 | INFO | Validation loss: 0.398, acc: 0.856, F1: 0.475
| 2021-07-13 11:32:10 | INFO | Start epoch 8:
| 2021-07-13 11:32:10 | INFO | Train Loss: 0.420, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 11:33:14 | INFO | Validation tp: 19, fn: 123, fp: 35, tn: 824
| 2021-07-13 11:33:14 | INFO | Validation loss: 0.401, acc: 0.842, F1: 0.553
| 2021-07-13 11:33:14 | INFO | Start epoch 9:
| 2021-07-13 11:33:15 | INFO | Train Loss: 0.287, tp: 0, fn: 6, fp: 1, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-13 11:34:19 | INFO | Validation tp: 10, fn: 132, fp: 20, tn: 839
| 2021-07-13 11:34:19 | INFO | Validation loss: 0.416, acc: 0.848, F1: 0.517
| 2021-07-13 11:34:19 | INFO | Start epoch 10:
| 2021-07-13 11:34:20 | INFO | Train Loss: 0.423, tp: 3, fn: 12, fp: 0, tn: 49, Acc: 0.812, Prec: 1.000, Rec: 0.200, F1: 0.612
| 2021-07-13 11:35:24 | INFO | Validation tp: 21, fn: 121, fp: 26, tn: 833
| 2021-07-13 11:35:24 | INFO | Validation loss: 0.458, acc: 0.853, F1: 0.571
| 2021-07-13 11:35:24 | INFO | Start epoch 11:
| 2021-07-13 11:35:25 | INFO | Train Loss: 0.216, tp: 4, fn: 6, fp: 0, tn: 54, Acc: 0.906, Prec: 1.000, Rec: 0.400, F1: 0.759
| 2021-07-13 11:36:29 | INFO | Validation tp: 36, fn: 106, fp: 69, tn: 790
| 2021-07-13 11:36:29 | INFO | Validation loss: 0.488, acc: 0.825, F1: 0.596
| 2021-07-13 11:36:29 | INFO | Start epoch 12:
| 2021-07-13 11:36:29 | INFO | Train Loss: 0.287, tp: 7, fn: 5, fp: 2, tn: 50, Acc: 0.891, Prec: 0.778, Rec: 0.583, F1: 0.801
| 2021-07-13 11:37:34 | INFO | Validation tp: 23, fn: 119, fp: 36, tn: 823
| 2021-07-13 11:37:34 | INFO | Validation loss: 0.499, acc: 0.845, F1: 0.571
| 2021-07-13 11:37:34 | INFO | Start epoch 13:
| 2021-07-13 11:37:35 | INFO | Train Loss: 0.242, tp: 7, fn: 4, fp: 0, tn: 53, Acc: 0.938, Prec: 1.000, Rec: 0.636, F1: 0.871
| 2021-07-13 11:38:39 | INFO | Validation tp: 31, fn: 111, fp: 56, tn: 803
| 2021-07-13 11:38:39 | INFO | Validation loss: 0.541, acc: 0.833, F1: 0.588
| 2021-07-13 11:38:39 | INFO | Start epoch 14:
| 2021-07-13 11:38:39 | INFO | Train Loss: 0.356, tp: 9, fn: 10, fp: 0, tn: 45, Acc: 0.844, Prec: 1.000, Rec: 0.474, F1: 0.771
| 2021-07-13 11:39:44 | INFO | Validation tp: 29, fn: 113, fp: 50, tn: 809
| 2021-07-13 11:39:44 | INFO | Validation loss: 0.556, acc: 0.837, F1: 0.585
| 2021-07-13 11:39:44 | INFO | Start epoch 15:
| 2021-07-13 11:39:44 | INFO | Train Loss: 0.126, tp: 9, fn: 1, fp: 1, tn: 53, Acc: 0.969, Prec: 0.900, Rec: 0.900, F1: 0.941
| 2021-07-13 11:40:49 | INFO | Validation tp: 26, fn: 116, fp: 45, tn: 814
| 2021-07-13 11:40:49 | INFO | Validation loss: 0.565, acc: 0.839, F1: 0.577
| 2021-07-13 11:40:51 | INFO | 
==============================Start training==============================
| 2021-07-13 11:40:51 | INFO | Command Line Args:   --lr 3e-5 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.3
Defaults:
  --hidden_size:     256

| 2021-07-13 11:40:51 | INFO | 
lr: 3e-05

| 2021-07-13 11:41:08 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 11:41:08 | INFO | Start epoch 1:
| 2021-07-13 11:41:09 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 11:42:12 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:42:12 | INFO | Validation loss: 0.399, acc: 0.858, F1: 0.462
| 2021-07-13 11:42:12 | INFO | Start epoch 2:
| 2021-07-13 11:42:13 | INFO | Train Loss: 0.409, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 11:43:16 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:43:16 | INFO | Validation loss: 0.402, acc: 0.858, F1: 0.462
| 2021-07-13 11:43:16 | INFO | Start epoch 3:
| 2021-07-13 11:43:17 | INFO | Train Loss: 0.419, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 11:44:21 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:44:21 | INFO | Validation loss: 0.393, acc: 0.858, F1: 0.462
| 2021-07-13 11:44:21 | INFO | Start epoch 4:
| 2021-07-13 11:44:22 | INFO | Train Loss: 0.527, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 11:45:26 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:45:26 | INFO | Validation loss: 0.383, acc: 0.858, F1: 0.462
| 2021-07-13 11:45:26 | INFO | Start epoch 5:
| 2021-07-13 11:45:26 | INFO | Train Loss: 0.394, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 11:46:30 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:46:30 | INFO | Validation loss: 0.379, acc: 0.858, F1: 0.462
| 2021-07-13 11:46:30 | INFO | Start epoch 6:
| 2021-07-13 11:46:31 | INFO | Train Loss: 0.360, tp: 2, fn: 11, fp: 0, tn: 51, Acc: 0.828, Prec: 1.000, Rec: 0.154, F1: 0.585
| 2021-07-13 11:47:35 | INFO | Validation tp: 8, fn: 134, fp: 10, tn: 849
| 2021-07-13 11:47:35 | INFO | Validation loss: 0.416, acc: 0.856, F1: 0.511
| 2021-07-13 11:47:35 | INFO | Start epoch 7:
| 2021-07-13 11:47:36 | INFO | Train Loss: 0.449, tp: 1, fn: 13, fp: 1, tn: 49, Acc: 0.781, Prec: 0.500, Rec: 0.071, F1: 0.500
| 2021-07-13 11:48:40 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:48:40 | INFO | Validation loss: 0.427, acc: 0.858, F1: 0.462
| 2021-07-13 11:48:40 | INFO | Start epoch 8:
| 2021-07-13 11:48:41 | INFO | Train Loss: 0.474, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-13 11:49:45 | INFO | Validation tp: 16, fn: 126, fp: 26, tn: 833
| 2021-07-13 11:49:45 | INFO | Validation loss: 0.429, acc: 0.848, F1: 0.545
| 2021-07-13 11:49:45 | INFO | Start epoch 9:
| 2021-07-13 11:49:45 | INFO | Train Loss: 0.227, tp: 1, fn: 5, fp: 1, tn: 57, Acc: 0.906, Prec: 0.500, Rec: 0.167, F1: 0.600
| 2021-07-13 11:50:49 | INFO | Validation tp: 33, fn: 109, fp: 68, tn: 791
| 2021-07-13 11:50:49 | INFO | Validation loss: 0.426, acc: 0.823, F1: 0.585
| 2021-07-13 11:50:49 | INFO | Start epoch 10:
| 2021-07-13 11:50:50 | INFO | Train Loss: 0.367, tp: 6, fn: 9, fp: 1, tn: 48, Acc: 0.844, Prec: 0.857, Rec: 0.400, F1: 0.726
| 2021-07-13 11:51:55 | INFO | Validation tp: 34, fn: 108, fp: 55, tn: 804
| 2021-07-13 11:51:55 | INFO | Validation loss: 0.461, acc: 0.837, F1: 0.601
| 2021-07-13 11:51:55 | INFO | Start epoch 11:
| 2021-07-13 11:51:55 | INFO | Train Loss: 0.254, tp: 6, fn: 4, fp: 4, tn: 50, Acc: 0.875, Prec: 0.600, Rec: 0.600, F1: 0.763
| 2021-07-13 11:52:59 | INFO | Validation tp: 38, fn: 104, fp: 63, tn: 796
| 2021-07-13 11:52:59 | INFO | Validation loss: 0.530, acc: 0.833, F1: 0.609
| 2021-07-13 11:52:59 | INFO | Start epoch 12:
| 2021-07-13 11:53:00 | INFO | Train Loss: 0.112, tp: 11, fn: 1, fp: 1, tn: 51, Acc: 0.969, Prec: 0.917, Rec: 0.917, F1: 0.949
| 2021-07-13 11:54:04 | INFO | Validation tp: 28, fn: 114, fp: 45, tn: 814
| 2021-07-13 11:54:04 | INFO | Validation loss: 0.569, acc: 0.841, F1: 0.586
| 2021-07-13 11:54:04 | INFO | Start epoch 13:
| 2021-07-13 11:54:05 | INFO | Train Loss: 0.194, tp: 10, fn: 1, fp: 2, tn: 51, Acc: 0.953, Prec: 0.833, Rec: 0.909, F1: 0.920
| 2021-07-13 11:55:09 | INFO | Validation tp: 34, fn: 108, fp: 57, tn: 802
| 2021-07-13 11:55:09 | INFO | Validation loss: 0.649, acc: 0.835, F1: 0.599
| 2021-07-13 11:55:09 | INFO | Start epoch 14:
| 2021-07-13 11:55:09 | INFO | Train Loss: 0.258, tp: 16, fn: 3, fp: 2, tn: 43, Acc: 0.922, Prec: 0.889, Rec: 0.842, F1: 0.905
| 2021-07-13 11:56:13 | INFO | Validation tp: 27, fn: 115, fp: 40, tn: 819
| 2021-07-13 11:56:13 | INFO | Validation loss: 0.646, acc: 0.845, F1: 0.586
| 2021-07-13 11:56:13 | INFO | Start epoch 15:
| 2021-07-13 11:56:14 | INFO | Train Loss: 0.071, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-13 11:57:18 | INFO | Validation tp: 29, fn: 113, fp: 40, tn: 819
| 2021-07-13 11:57:18 | INFO | Validation loss: 0.660, acc: 0.847, F1: 0.595
| 2021-07-13 11:57:20 | INFO | 
==============================Start training==============================
| 2021-07-13 11:57:20 | INFO | Command Line Args:   --lr 4e-5 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev.tsv
  output_dir:        output/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
  warmup_ratio:      0.3
Defaults:
  --hidden_size:     256

| 2021-07-13 11:57:20 | INFO | 
lr: 4e-05

| 2021-07-13 11:57:38 | INFO | MonoTransQuestModel(
  (model): XLMRobertaForSequenceClassification(
    (roberta): RobertaModel(
      (embeddings): RobertaEmbeddings(
        (word_embeddings): Embedding(250018, 768)
        (position_embeddings): Embedding(514, 768, padding_idx=1)
        (token_type_embeddings): Embedding(1, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): RobertaEncoder(
        (layer): ModuleList(
          (0): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): RobertaLayer(
            (attention): RobertaAttention(
              (self): RobertaSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): RobertaSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): RobertaIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): RobertaOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
    )
    (classifier): RobertaClassificationHead(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
      (out_proj): Linear(in_features=768, out_features=2, bias=True)
    )
  )
)
| 2021-07-13 11:57:38 | INFO | Start epoch 1:
| 2021-07-13 11:57:38 | INFO | Train Loss: 0.549, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-13 11:58:43 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:58:43 | INFO | Validation loss: 0.407, acc: 0.858, F1: 0.462
| 2021-07-13 11:58:43 | INFO | Start epoch 2:
| 2021-07-13 11:58:43 | INFO | Train Loss: 0.386, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-13 11:59:47 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 11:59:47 | INFO | Validation loss: 0.389, acc: 0.858, F1: 0.462
| 2021-07-13 11:59:47 | INFO | Start epoch 3:
| 2021-07-13 11:59:48 | INFO | Train Loss: 0.427, tp: 0, fn: 9, fp: 0, tn: 55, Acc: 0.859, Prec: 0.000, Rec: 0.000, F1: 0.462
| 2021-07-13 12:00:52 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 12:00:52 | INFO | Validation loss: 0.381, acc: 0.858, F1: 0.462
| 2021-07-13 12:00:52 | INFO | Start epoch 4:
| 2021-07-13 12:00:53 | INFO | Train Loss: 0.519, tp: 0, fn: 18, fp: 0, tn: 46, Acc: 0.719, Prec: 0.000, Rec: 0.000, F1: 0.418
| 2021-07-13 12:01:56 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 12:01:56 | INFO | Validation loss: 0.374, acc: 0.858, F1: 0.462
| 2021-07-13 12:01:56 | INFO | Start epoch 5:
| 2021-07-13 12:01:57 | INFO | Train Loss: 0.361, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-13 12:03:01 | INFO | Validation tp: 0, fn: 142, fp: 0, tn: 859
| 2021-07-13 12:03:01 | INFO | Validation loss: 0.382, acc: 0.858, F1: 0.462
| 2021-07-13 12:03:01 | INFO | Start epoch 6:
| 2021-07-13 12:03:02 | INFO | Train Loss: 0.325, tp: 1, fn: 12, fp: 0, tn: 51, Acc: 0.812, Prec: 1.000, Rec: 0.077, F1: 0.519
| 2021-07-13 12:04:06 | INFO | Validation tp: 13, fn: 129, fp: 10, tn: 849
| 2021-07-13 12:04:06 | INFO | Validation loss: 0.417, acc: 0.861, F1: 0.541
| 2021-07-13 12:04:06 | INFO | Start epoch 7:
| 2021-07-13 12:04:06 | INFO | Train Loss: 0.362, tp: 4, fn: 10, fp: 1, tn: 49, Acc: 0.828, Prec: 0.800, Rec: 0.286, F1: 0.660
| 2021-07-13 12:05:10 | INFO | Validation tp: 7, fn: 135, fp: 6, tn: 853
| 2021-07-13 12:05:10 | INFO | Validation loss: 0.487, acc: 0.859, F1: 0.507
| 2021-07-13 12:05:10 | INFO | Start epoch 8:
| 2021-07-13 12:05:11 | INFO | Train Loss: 0.253, tp: 2, fn: 9, fp: 0, tn: 53, Acc: 0.859, Prec: 1.000, Rec: 0.182, F1: 0.615
| 2021-07-13 12:06:15 | INFO | Validation tp: 26, fn: 116, fp: 70, tn: 789
| 2021-07-13 12:06:15 | INFO | Validation loss: 0.511, acc: 0.814, F1: 0.557
| 2021-07-13 12:06:15 | INFO | Start epoch 9:
| 2021-07-13 12:06:16 | INFO | Train Loss: 0.092, tp: 4, fn: 2, fp: 0, tn: 58, Acc: 0.969, Prec: 1.000, Rec: 0.667, F1: 0.892
| 2021-07-13 12:07:20 | INFO | Validation tp: 30, fn: 112, fp: 60, tn: 799
| 2021-07-13 12:07:20 | INFO | Validation loss: 0.563, acc: 0.828, F1: 0.581
| 2021-07-13 12:07:20 | INFO | Start epoch 10:
| 2021-07-13 12:07:20 | INFO | Train Loss: 0.157, tp: 14, fn: 1, fp: 1, tn: 48, Acc: 0.969, Prec: 0.933, Rec: 0.933, F1: 0.956
| 2021-07-13 12:08:24 | INFO | Validation tp: 26, fn: 116, fp: 55, tn: 804
| 2021-07-13 12:08:24 | INFO | Validation loss: 0.667, acc: 0.829, F1: 0.569
| 2021-07-13 12:08:24 | INFO | Start epoch 11:
| 2021-07-13 12:08:25 | INFO | Train Loss: 0.136, tp: 9, fn: 1, fp: 2, tn: 52, Acc: 0.953, Prec: 0.818, Rec: 0.900, F1: 0.915
| 2021-07-13 12:09:29 | INFO | Validation tp: 35, fn: 107, fp: 76, tn: 783
| 2021-07-13 12:09:29 | INFO | Validation loss: 0.704, acc: 0.817, F1: 0.586
| 2021-07-13 12:09:29 | INFO | Start epoch 12:
| 2021-07-13 12:09:30 | INFO | Train Loss: 0.093, tp: 12, fn: 0, fp: 1, tn: 51, Acc: 0.984, Prec: 0.923, Rec: 1.000, F1: 0.975
| 2021-07-13 12:10:34 | INFO | Validation tp: 15, fn: 127, fp: 34, tn: 825
| 2021-07-13 12:10:34 | INFO | Validation loss: 0.747, acc: 0.839, F1: 0.534
| 2021-07-13 12:10:34 | INFO | Start epoch 13:
| 2021-07-13 12:10:34 | INFO | Train Loss: 0.091, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-13 12:11:38 | INFO | Validation tp: 32, fn: 110, fp: 65, tn: 794
| 2021-07-13 12:11:38 | INFO | Validation loss: 0.829, acc: 0.825, F1: 0.584
| 2021-07-13 12:11:38 | INFO | Start epoch 14:
| 2021-07-13 12:11:39 | INFO | Train Loss: 0.006, tp: 19, fn: 0, fp: 0, tn: 45, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 12:12:43 | INFO | Validation tp: 27, fn: 115, fp: 47, tn: 812
| 2021-07-13 12:12:43 | INFO | Validation loss: 0.846, acc: 0.838, F1: 0.580
| 2021-07-13 12:12:43 | INFO | Start epoch 15:
| 2021-07-13 12:12:43 | INFO | Train Loss: 0.005, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-13 12:13:48 | INFO | Validation tp: 23, fn: 119, fp: 46, tn: 813
| 2021-07-13 12:13:48 | INFO | Validation loss: 0.845, acc: 0.835, F1: 0.563
