| 2021-07-20 05:44:17 | INFO | 
==============================Start training==============================
| 2021-07-20 05:44:17 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.2 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 05:44:17 | INFO | 
lr: 2e-05

| 2021-07-20 05:44:30 | INFO | Start epoch 1:
| 2021-07-20 05:44:31 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 05:45:37 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 05:45:37 | INFO | Validation loss: 0.397, acc: 0.859, F1: 0.462
| 2021-07-20 05:45:37 | INFO | Start epoch 2:
| 2021-07-20 05:45:38 | INFO | Train Loss: 0.375, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 05:46:45 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 05:46:45 | INFO | Validation loss: 0.387, acc: 0.859, F1: 0.462
| 2021-07-20 05:46:45 | INFO | Start epoch 3:
| 2021-07-20 05:46:46 | INFO | Train Loss: 0.388, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 05:47:53 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 05:47:53 | INFO | Validation loss: 0.377, acc: 0.859, F1: 0.462
| 2021-07-20 05:47:53 | INFO | Start epoch 4:
| 2021-07-20 05:47:54 | INFO | Train Loss: 0.481, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:02:16 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:02:16 | INFO | Validation loss: 0.369, acc: 0.859, F1: 0.462
| 2021-07-20 06:02:16 | INFO | Start epoch 5:
| 2021-07-20 06:02:17 | INFO | Train Loss: 0.413, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:03:22 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:03:22 | INFO | Validation loss: 0.386, acc: 0.859, F1: 0.462
| 2021-07-20 06:03:22 | INFO | Start epoch 6:
| 2021-07-20 06:03:22 | INFO | Train Loss: 0.341, tp: 1, fn: 11, fp: 1, tn: 51, Acc: 0.812, Prec: 0.500, Rec: 0.083, F1: 0.519
| 2021-07-20 06:04:29 | INFO | Validation tp: 1, fn: 140, fp: 1, tn: 858
| 2021-07-20 06:04:29 | INFO | Validation loss: 0.396, acc: 0.859, F1: 0.469
| 2021-07-20 06:04:29 | INFO | Start epoch 7:
| 2021-07-20 06:04:29 | INFO | Train Loss: 0.303, tp: 2, fn: 10, fp: 0, tn: 52, Acc: 0.844, Prec: 1.000, Rec: 0.167, F1: 0.599
| 2021-07-20 06:05:35 | INFO | Validation tp: 24, fn: 117, fp: 43, tn: 816
| 2021-07-20 06:05:35 | INFO | Validation loss: 0.411, acc: 0.840, F1: 0.571
| 2021-07-20 06:05:35 | INFO | Start epoch 8:
| 2021-07-20 06:05:36 | INFO | Train Loss: 0.392, tp: 9, fn: 7, fp: 5, tn: 43, Acc: 0.812, Prec: 0.643, Rec: 0.562, F1: 0.739
| 2021-07-20 06:06:42 | INFO | Validation tp: 48, fn: 93, fp: 107, tn: 752
| 2021-07-20 06:06:42 | INFO | Validation loss: 0.503, acc: 0.800, F1: 0.603
| 2021-07-20 06:06:42 | INFO | Start epoch 9:
| 2021-07-20 06:06:43 | INFO | Train Loss: 0.157, tp: 7, fn: 0, fp: 4, tn: 53, Acc: 0.938, Prec: 0.636, Rec: 1.000, F1: 0.871
| 2021-07-20 06:07:49 | INFO | Validation tp: 51, fn: 90, fp: 126, tn: 733
| 2021-07-20 06:07:49 | INFO | Validation loss: 0.589, acc: 0.784, F1: 0.596
| 2021-07-20 06:07:49 | INFO | Start epoch 10:
| 2021-07-20 06:07:49 | INFO | Train Loss: 0.204, tp: 12, fn: 2, fp: 3, tn: 47, Acc: 0.922, Prec: 0.800, Rec: 0.857, F1: 0.889
| 2021-07-20 06:08:55 | INFO | Validation tp: 15, fn: 126, fp: 37, tn: 822
| 2021-07-20 06:08:55 | INFO | Validation loss: 0.552, acc: 0.837, F1: 0.533
| 2021-07-20 06:08:55 | INFO | Start epoch 11:
| 2021-07-20 06:08:56 | INFO | Train Loss: 0.256, tp: 10, fn: 4, fp: 0, tn: 50, Acc: 0.938, Prec: 1.000, Rec: 0.714, F1: 0.897
| 2021-07-20 06:10:02 | INFO | Validation tp: 24, fn: 117, fp: 63, tn: 796
| 2021-07-20 06:10:02 | INFO | Validation loss: 0.654, acc: 0.820, F1: 0.554
| 2021-07-20 06:10:02 | INFO | Start epoch 12:
| 2021-07-20 06:10:02 | INFO | Train Loss: 0.020, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:11:09 | INFO | Validation tp: 19, fn: 122, fp: 47, tn: 812
| 2021-07-20 06:11:09 | INFO | Validation loss: 0.699, acc: 0.831, F1: 0.545
| 2021-07-20 06:11:09 | INFO | Start epoch 13:
| 2021-07-20 06:11:09 | INFO | Train Loss: 0.142, tp: 9, fn: 2, fp: 0, tn: 53, Acc: 0.969, Prec: 1.000, Rec: 0.818, F1: 0.941
| 2021-07-20 06:12:15 | INFO | Validation tp: 17, fn: 124, fp: 51, tn: 808
| 2021-07-20 06:12:15 | INFO | Validation loss: 0.690, acc: 0.825, F1: 0.532
| 2021-07-20 06:12:15 | INFO | Start epoch 14:
| 2021-07-20 06:12:16 | INFO | Train Loss: 0.134, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-20 06:13:22 | INFO | Validation tp: 17, fn: 124, fp: 46, tn: 813
| 2021-07-20 06:13:22 | INFO | Validation loss: 0.726, acc: 0.830, F1: 0.536
| 2021-07-20 06:13:22 | INFO | Start epoch 15:
| 2021-07-20 06:13:22 | INFO | Train Loss: 0.021, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:14:28 | INFO | Validation tp: 20, fn: 121, fp: 61, tn: 798
| 2021-07-20 06:14:28 | INFO | Validation loss: 0.745, acc: 0.818, F1: 0.539
| 2021-07-20 06:14:28 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 06:14:31 | INFO | 
==============================Start training==============================
| 2021-07-20 06:14:31 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.2 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 06:14:31 | INFO | 
lr: 3e-05

| 2021-07-20 06:14:43 | INFO | Start epoch 1:
| 2021-07-20 06:14:44 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 06:15:50 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:15:50 | INFO | Validation loss: 0.420, acc: 0.859, F1: 0.462
| 2021-07-20 06:15:50 | INFO | Start epoch 2:
| 2021-07-20 06:15:51 | INFO | Train Loss: 0.379, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:16:58 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:16:58 | INFO | Validation loss: 0.415, acc: 0.859, F1: 0.462
| 2021-07-20 06:16:58 | INFO | Start epoch 3:
| 2021-07-20 06:16:58 | INFO | Train Loss: 0.399, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:18:05 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:18:05 | INFO | Validation loss: 0.373, acc: 0.859, F1: 0.462
| 2021-07-20 06:18:05 | INFO | Start epoch 4:
| 2021-07-20 06:18:05 | INFO | Train Loss: 0.390, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:19:12 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:19:12 | INFO | Validation loss: 0.383, acc: 0.859, F1: 0.462
| 2021-07-20 06:19:12 | INFO | Start epoch 5:
| 2021-07-20 06:19:13 | INFO | Train Loss: 0.416, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:20:19 | INFO | Validation tp: 4, fn: 137, fp: 0, tn: 859
| 2021-07-20 06:20:19 | INFO | Validation loss: 0.382, acc: 0.863, F1: 0.491
| 2021-07-20 06:20:19 | INFO | Start epoch 6:
| 2021-07-20 06:20:20 | INFO | Train Loss: 0.328, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 06:21:26 | INFO | Validation tp: 23, fn: 118, fp: 41, tn: 818
| 2021-07-20 06:21:26 | INFO | Validation loss: 0.394, acc: 0.841, F1: 0.568
| 2021-07-20 06:21:26 | INFO | Start epoch 7:
| 2021-07-20 06:21:27 | INFO | Train Loss: 0.342, tp: 5, fn: 7, fp: 1, tn: 51, Acc: 0.875, Prec: 0.833, Rec: 0.417, F1: 0.741
| 2021-07-20 06:22:33 | INFO | Validation tp: 36, fn: 105, fp: 69, tn: 790
| 2021-07-20 06:22:33 | INFO | Validation loss: 0.431, acc: 0.826, F1: 0.597
| 2021-07-20 06:22:33 | INFO | Start epoch 8:
| 2021-07-20 06:22:34 | INFO | Train Loss: 0.345, tp: 11, fn: 5, fp: 3, tn: 45, Acc: 0.875, Prec: 0.786, Rec: 0.688, F1: 0.826
| 2021-07-20 06:23:41 | INFO | Validation tp: 30, fn: 111, fp: 68, tn: 791
| 2021-07-20 06:23:41 | INFO | Validation loss: 0.521, acc: 0.821, F1: 0.575
| 2021-07-20 06:23:41 | INFO | Start epoch 9:
| 2021-07-20 06:23:41 | INFO | Train Loss: 0.058, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:24:48 | INFO | Validation tp: 42, fn: 99, fp: 84, tn: 775
| 2021-07-20 06:24:48 | INFO | Validation loss: 0.563, acc: 0.817, F1: 0.605
| 2021-07-20 06:24:48 | INFO | Start epoch 10:
| 2021-07-20 06:24:48 | INFO | Train Loss: 0.074, tp: 13, fn: 1, fp: 1, tn: 49, Acc: 0.969, Prec: 0.929, Rec: 0.929, F1: 0.954
| 2021-07-20 06:25:56 | INFO | Validation tp: 48, fn: 93, fp: 96, tn: 763
| 2021-07-20 06:25:56 | INFO | Validation loss: 0.588, acc: 0.811, F1: 0.613
| 2021-07-20 06:25:56 | INFO | Start epoch 11:
| 2021-07-20 06:25:56 | INFO | Train Loss: 0.167, tp: 12, fn: 2, fp: 0, tn: 50, Acc: 0.969, Prec: 1.000, Rec: 0.857, F1: 0.952
| 2021-07-20 06:27:03 | INFO | Validation tp: 40, fn: 101, fp: 78, tn: 781
| 2021-07-20 06:27:03 | INFO | Validation loss: 0.676, acc: 0.821, F1: 0.603
| 2021-07-20 06:27:03 | INFO | Start epoch 12:
| 2021-07-20 06:27:03 | INFO | Train Loss: 0.088, tp: 7, fn: 1, fp: 1, tn: 55, Acc: 0.969, Prec: 0.875, Rec: 0.875, F1: 0.929
| 2021-07-20 06:28:10 | INFO | Validation tp: 32, fn: 109, fp: 58, tn: 801
| 2021-07-20 06:28:10 | INFO | Validation loss: 0.680, acc: 0.833, F1: 0.591
| 2021-07-20 06:28:10 | INFO | Start epoch 13:
| 2021-07-20 06:28:11 | INFO | Train Loss: 0.097, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-20 06:29:18 | INFO | Validation tp: 34, fn: 107, fp: 56, tn: 803
| 2021-07-20 06:29:18 | INFO | Validation loss: 0.720, acc: 0.837, F1: 0.601
| 2021-07-20 06:29:18 | INFO | Start epoch 14:
| 2021-07-20 06:29:19 | INFO | Train Loss: 0.027, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-20 06:30:26 | INFO | Validation tp: 33, fn: 108, fp: 51, tn: 808
| 2021-07-20 06:30:26 | INFO | Validation loss: 0.733, acc: 0.841, F1: 0.602
| 2021-07-20 06:30:26 | INFO | Start epoch 15:
| 2021-07-20 06:30:26 | INFO | Train Loss: 0.007, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:31:33 | INFO | Validation tp: 32, fn: 109, fp: 54, tn: 805
| 2021-07-20 06:31:33 | INFO | Validation loss: 0.754, acc: 0.837, F1: 0.595
| 2021-07-20 06:31:33 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 06:31:36 | INFO | 
==============================Start training==============================
| 2021-07-20 06:31:36 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.2 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 06:31:36 | INFO | 
lr: 4e-05

| 2021-07-20 06:31:48 | INFO | Start epoch 1:
| 2021-07-20 06:31:48 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 06:32:55 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:32:55 | INFO | Validation loss: 0.412, acc: 0.859, F1: 0.462
| 2021-07-20 06:32:55 | INFO | Start epoch 2:
| 2021-07-20 06:32:56 | INFO | Train Loss: 0.381, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:34:02 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:34:02 | INFO | Validation loss: 0.377, acc: 0.859, F1: 0.462
| 2021-07-20 06:34:02 | INFO | Start epoch 3:
| 2021-07-20 06:34:03 | INFO | Train Loss: 0.370, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:35:10 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:35:10 | INFO | Validation loss: 0.374, acc: 0.859, F1: 0.462
| 2021-07-20 06:35:10 | INFO | Start epoch 4:
| 2021-07-20 06:35:10 | INFO | Train Loss: 0.409, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 06:36:17 | INFO | Validation tp: 1, fn: 140, fp: 5, tn: 854
| 2021-07-20 06:36:17 | INFO | Validation loss: 0.392, acc: 0.855, F1: 0.468
| 2021-07-20 06:36:17 | INFO | Start epoch 5:
| 2021-07-20 06:36:17 | INFO | Train Loss: 0.361, tp: 1, fn: 11, fp: 1, tn: 51, Acc: 0.812, Prec: 0.500, Rec: 0.083, F1: 0.519
| 2021-07-20 06:37:25 | INFO | Validation tp: 31, fn: 110, fp: 37, tn: 822
| 2021-07-20 06:37:25 | INFO | Validation loss: 0.412, acc: 0.853, F1: 0.607
| 2021-07-20 06:37:25 | INFO | Start epoch 6:
| 2021-07-20 06:37:25 | INFO | Train Loss: 0.243, tp: 5, fn: 7, fp: 0, tn: 52, Acc: 0.891, Prec: 1.000, Rec: 0.417, F1: 0.763
| 2021-07-20 06:38:32 | INFO | Validation tp: 25, fn: 116, fp: 73, tn: 786
| 2021-07-20 06:38:32 | INFO | Validation loss: 0.447, acc: 0.811, F1: 0.551
| 2021-07-20 06:38:32 | INFO | Start epoch 7:
| 2021-07-20 06:38:33 | INFO | Train Loss: 0.175, tp: 10, fn: 2, fp: 0, tn: 52, Acc: 0.969, Prec: 1.000, Rec: 0.833, F1: 0.945
| 2021-07-20 06:39:39 | INFO | Validation tp: 26, fn: 115, fp: 50, tn: 809
| 2021-07-20 06:39:39 | INFO | Validation loss: 0.466, acc: 0.835, F1: 0.574
| 2021-07-20 06:39:39 | INFO | Start epoch 8:
| 2021-07-20 06:39:40 | INFO | Train Loss: 0.225, tp: 11, fn: 5, fp: 1, tn: 47, Acc: 0.906, Prec: 0.917, Rec: 0.688, F1: 0.863
| 2021-07-20 06:40:47 | INFO | Validation tp: 26, fn: 115, fp: 45, tn: 814
| 2021-07-20 06:40:47 | INFO | Validation loss: 0.571, acc: 0.840, F1: 0.578
| 2021-07-20 06:40:47 | INFO | Start epoch 9:
| 2021-07-20 06:40:47 | INFO | Train Loss: 0.047, tp: 6, fn: 1, fp: 0, tn: 57, Acc: 0.984, Prec: 1.000, Rec: 0.857, F1: 0.957
| 2021-07-20 06:41:54 | INFO | Validation tp: 31, fn: 110, fp: 43, tn: 816
| 2021-07-20 06:41:54 | INFO | Validation loss: 0.665, acc: 0.847, F1: 0.601
| 2021-07-20 06:41:54 | INFO | Start epoch 10:
| 2021-07-20 06:41:54 | INFO | Train Loss: 0.103, tp: 13, fn: 1, fp: 0, tn: 50, Acc: 0.984, Prec: 1.000, Rec: 0.929, F1: 0.977
| 2021-07-20 06:43:01 | INFO | Validation tp: 39, fn: 102, fp: 103, tn: 756
| 2021-07-20 06:43:01 | INFO | Validation loss: 0.741, acc: 0.795, F1: 0.578
| 2021-07-20 06:43:01 | INFO | Start epoch 11:
| 2021-07-20 06:43:02 | INFO | Train Loss: 0.204, tp: 12, fn: 2, fp: 1, tn: 49, Acc: 0.953, Prec: 0.923, Rec: 0.857, F1: 0.930
| 2021-07-20 06:44:09 | INFO | Validation tp: 27, fn: 114, fp: 58, tn: 801
| 2021-07-20 06:44:09 | INFO | Validation loss: 0.768, acc: 0.828, F1: 0.571
| 2021-07-20 06:44:09 | INFO | Start epoch 12:
| 2021-07-20 06:44:09 | INFO | Train Loss: 0.008, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:45:16 | INFO | Validation tp: 20, fn: 121, fp: 45, tn: 814
| 2021-07-20 06:45:16 | INFO | Validation loss: 0.803, acc: 0.834, F1: 0.551
| 2021-07-20 06:45:16 | INFO | Start epoch 13:
| 2021-07-20 06:45:16 | INFO | Train Loss: 0.028, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-20 06:46:23 | INFO | Validation tp: 18, fn: 123, fp: 28, tn: 831
| 2021-07-20 06:46:23 | INFO | Validation loss: 0.796, acc: 0.849, F1: 0.555
| 2021-07-20 06:46:23 | INFO | Start epoch 14:
| 2021-07-20 06:46:24 | INFO | Train Loss: 0.021, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-20 06:47:31 | INFO | Validation tp: 25, fn: 116, fp: 49, tn: 810
| 2021-07-20 06:47:31 | INFO | Validation loss: 0.823, acc: 0.835, F1: 0.570
| 2021-07-20 06:47:31 | INFO | Start epoch 15:
| 2021-07-20 06:47:31 | INFO | Train Loss: 0.004, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 06:48:38 | INFO | Validation tp: 21, fn: 120, fp: 36, tn: 823
| 2021-07-20 06:48:38 | INFO | Validation loss: 0.821, acc: 0.844, F1: 0.563
| 2021-07-20 06:48:38 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 06:48:40 | INFO | 
==============================Start training==============================
| 2021-07-20 06:48:40 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 06:48:40 | INFO | 
lr: 2e-05

| 2021-07-20 06:48:52 | INFO | Start epoch 1:
| 2021-07-20 06:48:53 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 06:50:00 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:50:00 | INFO | Validation loss: 0.398, acc: 0.859, F1: 0.462
| 2021-07-20 06:50:00 | INFO | Start epoch 2:
| 2021-07-20 06:50:00 | INFO | Train Loss: 0.378, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:51:07 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:51:07 | INFO | Validation loss: 0.407, acc: 0.859, F1: 0.462
| 2021-07-20 06:51:07 | INFO | Start epoch 3:
| 2021-07-20 06:51:08 | INFO | Train Loss: 0.413, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 06:52:14 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:52:14 | INFO | Validation loss: 0.373, acc: 0.859, F1: 0.462
| 2021-07-20 06:52:14 | INFO | Start epoch 4:
| 2021-07-20 06:52:15 | INFO | Train Loss: 0.438, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:53:22 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:53:22 | INFO | Validation loss: 0.362, acc: 0.859, F1: 0.462
| 2021-07-20 06:53:22 | INFO | Start epoch 5:
| 2021-07-20 06:53:23 | INFO | Train Loss: 0.433, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 06:54:29 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 06:54:29 | INFO | Validation loss: 0.375, acc: 0.859, F1: 0.462
| 2021-07-20 06:54:29 | INFO | Start epoch 6:
| 2021-07-20 06:54:30 | INFO | Train Loss: 0.363, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 06:55:37 | INFO | Validation tp: 1, fn: 140, fp: 2, tn: 857
| 2021-07-20 06:55:37 | INFO | Validation loss: 0.389, acc: 0.858, F1: 0.469
| 2021-07-20 06:55:37 | INFO | Start epoch 7:
| 2021-07-20 06:55:37 | INFO | Train Loss: 0.316, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 06:56:45 | INFO | Validation tp: 41, fn: 100, fp: 67, tn: 792
| 2021-07-20 06:56:45 | INFO | Validation loss: 0.401, acc: 0.833, F1: 0.617
| 2021-07-20 06:56:45 | INFO | Start epoch 8:
| 2021-07-20 06:56:45 | INFO | Train Loss: 0.344, tp: 11, fn: 5, fp: 10, tn: 38, Acc: 0.766, Prec: 0.524, Rec: 0.688, F1: 0.715
| 2021-07-20 06:57:52 | INFO | Validation tp: 41, fn: 100, fp: 94, tn: 765
| 2021-07-20 06:57:52 | INFO | Validation loss: 0.466, acc: 0.806, F1: 0.592
| 2021-07-20 06:57:52 | INFO | Start epoch 9:
| 2021-07-20 06:57:52 | INFO | Train Loss: 0.224, tp: 6, fn: 1, fp: 5, tn: 52, Acc: 0.906, Prec: 0.545, Rec: 0.857, F1: 0.806
| 2021-07-20 06:58:59 | INFO | Validation tp: 48, fn: 93, fp: 123, tn: 736
| 2021-07-20 06:58:59 | INFO | Validation loss: 0.614, acc: 0.784, F1: 0.590
| 2021-07-20 06:58:59 | INFO | Start epoch 10:
| 2021-07-20 06:58:59 | INFO | Train Loss: 0.182, tp: 14, fn: 0, fp: 4, tn: 46, Acc: 0.938, Prec: 0.778, Rec: 1.000, F1: 0.917
| 2021-07-20 07:00:06 | INFO | Validation tp: 28, fn: 113, fp: 59, tn: 800
| 2021-07-20 07:00:06 | INFO | Validation loss: 0.572, acc: 0.828, F1: 0.574
| 2021-07-20 07:00:06 | INFO | Start epoch 11:
| 2021-07-20 07:00:07 | INFO | Train Loss: 0.218, tp: 10, fn: 4, fp: 0, tn: 50, Acc: 0.938, Prec: 1.000, Rec: 0.714, F1: 0.897
| 2021-07-20 07:01:13 | INFO | Validation tp: 28, fn: 113, fp: 73, tn: 786
| 2021-07-20 07:01:13 | INFO | Validation loss: 0.650, acc: 0.814, F1: 0.563
| 2021-07-20 07:01:13 | INFO | Start epoch 12:
| 2021-07-20 07:01:14 | INFO | Train Loss: 0.026, tp: 8, fn: 0, fp: 1, tn: 55, Acc: 0.984, Prec: 0.889, Rec: 1.000, F1: 0.966
| 2021-07-20 07:02:21 | INFO | Validation tp: 21, fn: 120, fp: 44, tn: 815
| 2021-07-20 07:02:21 | INFO | Validation loss: 0.669, acc: 0.836, F1: 0.556
| 2021-07-20 07:02:21 | INFO | Start epoch 13:
| 2021-07-20 07:02:21 | INFO | Train Loss: 0.120, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-20 07:03:28 | INFO | Validation tp: 22, fn: 119, fp: 44, tn: 815
| 2021-07-20 07:03:28 | INFO | Validation loss: 0.710, acc: 0.837, F1: 0.561
| 2021-07-20 07:03:28 | INFO | Start epoch 14:
| 2021-07-20 07:03:29 | INFO | Train Loss: 0.055, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-20 07:04:36 | INFO | Validation tp: 24, fn: 117, fp: 53, tn: 806
| 2021-07-20 07:04:36 | INFO | Validation loss: 0.740, acc: 0.830, F1: 0.562
| 2021-07-20 07:04:36 | INFO | Start epoch 15:
| 2021-07-20 07:04:37 | INFO | Train Loss: 0.012, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 07:05:43 | INFO | Validation tp: 23, fn: 118, fp: 53, tn: 806
| 2021-07-20 07:05:43 | INFO | Validation loss: 0.744, acc: 0.829, F1: 0.558
| 2021-07-20 07:05:43 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 07:05:45 | INFO | 
==============================Start training==============================
| 2021-07-20 07:05:45 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 07:05:45 | INFO | 
lr: 3e-05

| 2021-07-20 07:05:58 | INFO | Start epoch 1:
| 2021-07-20 07:05:58 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 07:07:05 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:07:05 | INFO | Validation loss: 0.397, acc: 0.859, F1: 0.462
| 2021-07-20 07:07:05 | INFO | Start epoch 2:
| 2021-07-20 07:07:05 | INFO | Train Loss: 0.375, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:08:12 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:08:12 | INFO | Validation loss: 0.387, acc: 0.859, F1: 0.462
| 2021-07-20 07:08:12 | INFO | Start epoch 3:
| 2021-07-20 07:08:13 | INFO | Train Loss: 0.388, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:09:20 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:09:20 | INFO | Validation loss: 0.377, acc: 0.859, F1: 0.462
| 2021-07-20 07:09:20 | INFO | Start epoch 4:
| 2021-07-20 07:09:20 | INFO | Train Loss: 0.481, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:10:27 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:10:27 | INFO | Validation loss: 0.378, acc: 0.859, F1: 0.462
| 2021-07-20 07:10:27 | INFO | Start epoch 5:
| 2021-07-20 07:10:27 | INFO | Train Loss: 0.507, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:11:35 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:11:35 | INFO | Validation loss: 0.370, acc: 0.859, F1: 0.462
| 2021-07-20 07:11:35 | INFO | Start epoch 6:
| 2021-07-20 07:11:35 | INFO | Train Loss: 0.360, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:12:42 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:12:42 | INFO | Validation loss: 0.437, acc: 0.859, F1: 0.462
| 2021-07-20 07:12:42 | INFO | Start epoch 7:
| 2021-07-20 07:12:43 | INFO | Train Loss: 0.474, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:13:50 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:13:50 | INFO | Validation loss: 0.409, acc: 0.859, F1: 0.462
| 2021-07-20 07:13:50 | INFO | Start epoch 8:
| 2021-07-20 07:13:50 | INFO | Train Loss: 0.593, tp: 0, fn: 16, fp: 0, tn: 48, Acc: 0.750, Prec: 0.000, Rec: 0.000, F1: 0.429
| 2021-07-20 07:14:57 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:14:57 | INFO | Validation loss: 0.410, acc: 0.859, F1: 0.462
| 2021-07-20 07:14:57 | INFO | Start epoch 9:
| 2021-07-20 07:14:58 | INFO | Train Loss: 0.355, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 07:16:05 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:16:05 | INFO | Validation loss: 0.414, acc: 0.859, F1: 0.462
| 2021-07-20 07:16:05 | INFO | Start epoch 10:
| 2021-07-20 07:16:05 | INFO | Train Loss: 0.531, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-20 07:17:12 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:17:12 | INFO | Validation loss: 0.409, acc: 0.859, F1: 0.462
| 2021-07-20 07:17:12 | INFO | Start epoch 11:
| 2021-07-20 07:17:13 | INFO | Train Loss: 0.539, tp: 0, fn: 14, fp: 0, tn: 50, Acc: 0.781, Prec: 0.000, Rec: 0.000, F1: 0.439
| 2021-07-20 07:18:20 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:18:20 | INFO | Validation loss: 0.410, acc: 0.859, F1: 0.462
| 2021-07-20 07:18:20 | INFO | Start epoch 12:
| 2021-07-20 07:18:20 | INFO | Train Loss: 0.384, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:19:28 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:19:28 | INFO | Validation loss: 0.411, acc: 0.859, F1: 0.462
| 2021-07-20 07:19:28 | INFO | Start epoch 13:
| 2021-07-20 07:19:28 | INFO | Train Loss: 0.464, tp: 0, fn: 11, fp: 0, tn: 53, Acc: 0.828, Prec: 0.000, Rec: 0.000, F1: 0.453
| 2021-07-20 07:20:35 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:20:35 | INFO | Validation loss: 0.410, acc: 0.859, F1: 0.462
| 2021-07-20 07:20:35 | INFO | Start epoch 14:
| 2021-07-20 07:20:36 | INFO | Train Loss: 0.432, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-20 07:21:42 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:21:42 | INFO | Validation loss: 0.410, acc: 0.859, F1: 0.462
| 2021-07-20 07:21:42 | INFO | Start epoch 15:
| 2021-07-20 07:21:43 | INFO | Train Loss: 0.383, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:22:49 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:22:49 | INFO | Validation loss: 0.410, acc: 0.859, F1: 0.462
| 2021-07-20 07:22:49 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 07:22:52 | INFO | 
==============================Start training==============================
| 2021-07-20 07:22:52 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.3 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 07:22:52 | INFO | 
lr: 4e-05

| 2021-07-20 07:23:04 | INFO | Start epoch 1:
| 2021-07-20 07:23:05 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 07:24:12 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:24:12 | INFO | Validation loss: 0.407, acc: 0.859, F1: 0.462
| 2021-07-20 07:24:12 | INFO | Start epoch 2:
| 2021-07-20 07:24:12 | INFO | Train Loss: 0.387, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:25:19 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:25:19 | INFO | Validation loss: 0.422, acc: 0.859, F1: 0.462
| 2021-07-20 07:25:19 | INFO | Start epoch 3:
| 2021-07-20 07:25:20 | INFO | Train Loss: 0.395, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:26:26 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:26:26 | INFO | Validation loss: 0.384, acc: 0.859, F1: 0.462
| 2021-07-20 07:26:26 | INFO | Start epoch 4:
| 2021-07-20 07:26:27 | INFO | Train Loss: 0.459, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:27:34 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:27:34 | INFO | Validation loss: 0.380, acc: 0.859, F1: 0.462
| 2021-07-20 07:27:34 | INFO | Start epoch 5:
| 2021-07-20 07:27:34 | INFO | Train Loss: 0.453, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:28:41 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:28:41 | INFO | Validation loss: 0.380, acc: 0.859, F1: 0.462
| 2021-07-20 07:28:41 | INFO | Start epoch 6:
| 2021-07-20 07:28:42 | INFO | Train Loss: 0.330, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 07:29:48 | INFO | Validation tp: 0, fn: 141, fp: 1, tn: 858
| 2021-07-20 07:29:48 | INFO | Validation loss: 0.396, acc: 0.858, F1: 0.462
| 2021-07-20 07:29:48 | INFO | Start epoch 7:
| 2021-07-20 07:29:49 | INFO | Train Loss: 0.352, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 07:30:55 | INFO | Validation tp: 40, fn: 101, fp: 61, tn: 798
| 2021-07-20 07:30:55 | INFO | Validation loss: 0.429, acc: 0.838, F1: 0.619
| 2021-07-20 07:30:55 | INFO | Start epoch 8:
| 2021-07-20 07:30:56 | INFO | Train Loss: 0.342, tp: 12, fn: 4, fp: 4, tn: 44, Acc: 0.875, Prec: 0.750, Rec: 0.750, F1: 0.833
| 2021-07-20 07:32:03 | INFO | Validation tp: 51, fn: 90, fp: 110, tn: 749
| 2021-07-20 07:32:03 | INFO | Validation loss: 0.486, acc: 0.800, F1: 0.610
| 2021-07-20 07:32:03 | INFO | Start epoch 9:
| 2021-07-20 07:32:03 | INFO | Train Loss: 0.122, tp: 7, fn: 0, fp: 3, tn: 54, Acc: 0.953, Prec: 0.700, Rec: 1.000, F1: 0.898
| 2021-07-20 07:33:10 | INFO | Validation tp: 47, fn: 94, fp: 99, tn: 760
| 2021-07-20 07:33:10 | INFO | Validation loss: 0.543, acc: 0.807, F1: 0.607
| 2021-07-20 07:33:10 | INFO | Start epoch 10:
| 2021-07-20 07:33:11 | INFO | Train Loss: 0.080, tp: 13, fn: 1, fp: 0, tn: 50, Acc: 0.984, Prec: 1.000, Rec: 0.929, F1: 0.977
| 2021-07-20 07:34:18 | INFO | Validation tp: 43, fn: 98, fp: 83, tn: 776
| 2021-07-20 07:34:18 | INFO | Validation loss: 0.645, acc: 0.819, F1: 0.609
| 2021-07-20 07:34:18 | INFO | Start epoch 11:
| 2021-07-20 07:34:18 | INFO | Train Loss: 0.100, tp: 13, fn: 1, fp: 0, tn: 50, Acc: 0.984, Prec: 1.000, Rec: 0.929, F1: 0.977
| 2021-07-20 07:35:25 | INFO | Validation tp: 43, fn: 98, fp: 100, tn: 759
| 2021-07-20 07:35:25 | INFO | Validation loss: 0.735, acc: 0.802, F1: 0.594
| 2021-07-20 07:35:25 | INFO | Start epoch 12:
| 2021-07-20 07:35:25 | INFO | Train Loss: 0.061, tp: 7, fn: 1, fp: 1, tn: 55, Acc: 0.969, Prec: 0.875, Rec: 0.875, F1: 0.929
| 2021-07-20 07:36:33 | INFO | Validation tp: 25, fn: 116, fp: 44, tn: 815
| 2021-07-20 07:36:33 | INFO | Validation loss: 0.717, acc: 0.840, F1: 0.574
| 2021-07-20 07:36:33 | INFO | Start epoch 13:
| 2021-07-20 07:36:33 | INFO | Train Loss: 0.085, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-20 07:37:40 | INFO | Validation tp: 28, fn: 113, fp: 62, tn: 797
| 2021-07-20 07:37:40 | INFO | Validation loss: 0.747, acc: 0.825, F1: 0.572
| 2021-07-20 07:37:40 | INFO | Start epoch 14:
| 2021-07-20 07:37:41 | INFO | Train Loss: 0.184, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-20 07:38:48 | INFO | Validation tp: 25, fn: 116, fp: 46, tn: 813
| 2021-07-20 07:38:48 | INFO | Validation loss: 0.788, acc: 0.838, F1: 0.573
| 2021-07-20 07:38:48 | INFO | Start epoch 15:
| 2021-07-20 07:38:48 | INFO | Train Loss: 0.014, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 07:39:55 | INFO | Validation tp: 25, fn: 116, fp: 51, tn: 808
| 2021-07-20 07:39:55 | INFO | Validation loss: 0.809, acc: 0.833, F1: 0.568
| 2021-07-20 07:39:55 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 07:39:58 | INFO | 
==============================Start training==============================
| 2021-07-20 07:39:58 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 07:39:58 | INFO | 
lr: 2e-05

| 2021-07-20 07:40:09 | INFO | Start epoch 1:
| 2021-07-20 07:40:10 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 07:41:16 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:41:16 | INFO | Validation loss: 0.404, acc: 0.859, F1: 0.462
| 2021-07-20 07:41:16 | INFO | Start epoch 2:
| 2021-07-20 07:41:17 | INFO | Train Loss: 0.398, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:42:24 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:42:24 | INFO | Validation loss: 0.409, acc: 0.859, F1: 0.462
| 2021-07-20 07:42:24 | INFO | Start epoch 3:
| 2021-07-20 07:42:24 | INFO | Train Loss: 0.412, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 07:43:31 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:43:31 | INFO | Validation loss: 0.382, acc: 0.859, F1: 0.462
| 2021-07-20 07:43:31 | INFO | Start epoch 4:
| 2021-07-20 07:43:32 | INFO | Train Loss: 0.453, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:44:39 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:44:39 | INFO | Validation loss: 0.373, acc: 0.859, F1: 0.462
| 2021-07-20 07:44:39 | INFO | Start epoch 5:
| 2021-07-20 07:44:39 | INFO | Train Loss: 0.408, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 07:45:46 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:45:46 | INFO | Validation loss: 0.370, acc: 0.859, F1: 0.462
| 2021-07-20 07:45:46 | INFO | Start epoch 6:
| 2021-07-20 07:45:46 | INFO | Train Loss: 0.380, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 07:46:53 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 07:46:53 | INFO | Validation loss: 0.386, acc: 0.859, F1: 0.462
| 2021-07-20 07:46:53 | INFO | Start epoch 7:
| 2021-07-20 07:46:54 | INFO | Train Loss: 0.329, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 07:48:01 | INFO | Validation tp: 21, fn: 120, fp: 25, tn: 834
| 2021-07-20 07:48:01 | INFO | Validation loss: 0.399, acc: 0.855, F1: 0.572
| 2021-07-20 07:48:01 | INFO | Start epoch 8:
| 2021-07-20 07:48:01 | INFO | Train Loss: 0.413, tp: 7, fn: 9, fp: 5, tn: 43, Acc: 0.781, Prec: 0.583, Rec: 0.438, F1: 0.680
| 2021-07-20 07:49:08 | INFO | Validation tp: 38, fn: 103, fp: 98, tn: 761
| 2021-07-20 07:49:08 | INFO | Validation loss: 0.455, acc: 0.799, F1: 0.579
| 2021-07-20 07:49:08 | INFO | Start epoch 9:
| 2021-07-20 07:49:09 | INFO | Train Loss: 0.173, tp: 7, fn: 0, fp: 3, tn: 54, Acc: 0.953, Prec: 0.700, Rec: 1.000, F1: 0.898
| 2021-07-20 07:50:16 | INFO | Validation tp: 39, fn: 102, fp: 87, tn: 772
| 2021-07-20 07:50:16 | INFO | Validation loss: 0.513, acc: 0.811, F1: 0.592
| 2021-07-20 07:50:16 | INFO | Start epoch 10:
| 2021-07-20 07:50:16 | INFO | Train Loss: 0.190, tp: 13, fn: 1, fp: 1, tn: 49, Acc: 0.969, Prec: 0.929, Rec: 0.929, F1: 0.954
| 2021-07-20 07:51:23 | INFO | Validation tp: 26, fn: 115, fp: 49, tn: 810
| 2021-07-20 07:51:23 | INFO | Validation loss: 0.546, acc: 0.836, F1: 0.574
| 2021-07-20 07:51:23 | INFO | Start epoch 11:
| 2021-07-20 07:51:24 | INFO | Train Loss: 0.230, tp: 10, fn: 4, fp: 1, tn: 49, Acc: 0.922, Prec: 0.909, Rec: 0.714, F1: 0.876
| 2021-07-20 07:52:31 | INFO | Validation tp: 21, fn: 120, fp: 51, tn: 808
| 2021-07-20 07:52:31 | INFO | Validation loss: 0.647, acc: 0.829, F1: 0.551
| 2021-07-20 07:52:31 | INFO | Start epoch 12:
| 2021-07-20 07:52:31 | INFO | Train Loss: 0.024, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 07:53:38 | INFO | Validation tp: 22, fn: 119, fp: 53, tn: 806
| 2021-07-20 07:53:38 | INFO | Validation loss: 0.701, acc: 0.828, F1: 0.554
| 2021-07-20 07:53:38 | INFO | Start epoch 13:
| 2021-07-20 07:53:39 | INFO | Train Loss: 0.073, tp: 10, fn: 1, fp: 0, tn: 53, Acc: 0.984, Prec: 1.000, Rec: 0.909, F1: 0.972
| 2021-07-20 07:54:46 | INFO | Validation tp: 20, fn: 121, fp: 46, tn: 813
| 2021-07-20 07:54:46 | INFO | Validation loss: 0.731, acc: 0.833, F1: 0.550
| 2021-07-20 07:54:46 | INFO | Start epoch 14:
| 2021-07-20 07:54:46 | INFO | Train Loss: 0.018, tp: 10, fn: 0, fp: 0, tn: 54, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 07:55:53 | INFO | Validation tp: 32, fn: 109, fp: 69, tn: 790
| 2021-07-20 07:55:53 | INFO | Validation loss: 0.835, acc: 0.822, F1: 0.582
| 2021-07-20 07:55:53 | INFO | Start epoch 15:
| 2021-07-20 07:55:54 | INFO | Train Loss: 0.033, tp: 8, fn: 0, fp: 1, tn: 55, Acc: 0.984, Prec: 0.889, Rec: 1.000, F1: 0.966
| 2021-07-20 07:57:00 | INFO | Validation tp: 28, fn: 113, fp: 59, tn: 800
| 2021-07-20 07:57:00 | INFO | Validation loss: 0.910, acc: 0.828, F1: 0.574
| 2021-07-20 07:57:00 | INFO | Start epoch 16:
| 2021-07-20 07:57:01 | INFO | Train Loss: 0.085, tp: 4, fn: 1, fp: 0, tn: 59, Acc: 0.984, Prec: 1.000, Rec: 0.800, F1: 0.940
| 2021-07-20 07:58:07 | INFO | Validation tp: 22, fn: 119, fp: 33, tn: 826
| 2021-07-20 07:58:07 | INFO | Validation loss: 0.828, acc: 0.848, F1: 0.570
| 2021-07-20 07:58:07 | INFO | Start epoch 17:
| 2021-07-20 07:58:08 | INFO | Train Loss: 0.005, tp: 17, fn: 0, fp: 0, tn: 47, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 07:59:15 | INFO | Validation tp: 29, fn: 112, fp: 49, tn: 810
| 2021-07-20 07:59:15 | INFO | Validation loss: 0.843, acc: 0.839, F1: 0.587
| 2021-07-20 07:59:15 | INFO | Start epoch 18:
| 2021-07-20 07:59:16 | INFO | Train Loss: 0.004, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:00:22 | INFO | Validation tp: 28, fn: 113, fp: 48, tn: 811
| 2021-07-20 08:00:22 | INFO | Validation loss: 0.891, acc: 0.839, F1: 0.584
| 2021-07-20 08:00:22 | INFO | Start epoch 19:
| 2021-07-20 08:00:23 | INFO | Train Loss: 0.005, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:01:30 | INFO | Validation tp: 27, fn: 114, fp: 49, tn: 810
| 2021-07-20 08:01:30 | INFO | Validation loss: 0.915, acc: 0.837, F1: 0.579
| 2021-07-20 08:01:30 | INFO | Start epoch 20:
| 2021-07-20 08:01:30 | INFO | Train Loss: 0.003, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:02:38 | INFO | Validation tp: 26, fn: 115, fp: 45, tn: 814
| 2021-07-20 08:02:38 | INFO | Validation loss: 0.929, acc: 0.840, F1: 0.578
| 2021-07-20 08:02:38 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 08:02:40 | INFO | 
==============================Start training==============================
| 2021-07-20 08:02:40 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 08:02:40 | INFO | 
lr: 3e-05

| 2021-07-20 08:02:53 | INFO | Start epoch 1:
| 2021-07-20 08:02:54 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 08:04:00 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:04:00 | INFO | Validation loss: 0.398, acc: 0.859, F1: 0.462
| 2021-07-20 08:04:00 | INFO | Start epoch 2:
| 2021-07-20 08:04:01 | INFO | Train Loss: 0.381, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:05:08 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:05:08 | INFO | Validation loss: 0.390, acc: 0.859, F1: 0.462
| 2021-07-20 08:05:08 | INFO | Start epoch 3:
| 2021-07-20 08:05:08 | INFO | Train Loss: 0.397, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:06:15 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:06:15 | INFO | Validation loss: 0.370, acc: 0.859, F1: 0.462
| 2021-07-20 08:06:15 | INFO | Start epoch 4:
| 2021-07-20 08:06:16 | INFO | Train Loss: 0.408, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:07:23 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:07:23 | INFO | Validation loss: 0.364, acc: 0.859, F1: 0.462
| 2021-07-20 08:07:23 | INFO | Start epoch 5:
| 2021-07-20 08:07:23 | INFO | Train Loss: 0.387, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:08:30 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:08:30 | INFO | Validation loss: 0.367, acc: 0.859, F1: 0.462
| 2021-07-20 08:08:30 | INFO | Start epoch 6:
| 2021-07-20 08:08:31 | INFO | Train Loss: 0.353, tp: 1, fn: 11, fp: 0, tn: 52, Acc: 0.828, Prec: 1.000, Rec: 0.083, F1: 0.529
| 2021-07-20 08:09:38 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:09:38 | INFO | Validation loss: 0.392, acc: 0.859, F1: 0.462
| 2021-07-20 08:09:38 | INFO | Start epoch 7:
| 2021-07-20 08:09:39 | INFO | Train Loss: 0.346, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:10:46 | INFO | Validation tp: 23, fn: 118, fp: 30, tn: 829
| 2021-07-20 08:10:46 | INFO | Validation loss: 0.400, acc: 0.852, F1: 0.578
| 2021-07-20 08:10:46 | INFO | Start epoch 8:
| 2021-07-20 08:10:46 | INFO | Train Loss: 0.381, tp: 7, fn: 9, fp: 4, tn: 44, Acc: 0.797, Prec: 0.636, Rec: 0.438, F1: 0.695
| 2021-07-20 08:11:53 | INFO | Validation tp: 69, fn: 72, fp: 138, tn: 721
| 2021-07-20 08:11:53 | INFO | Validation loss: 0.465, acc: 0.790, F1: 0.635
| 2021-07-20 08:11:53 | INFO | Start epoch 9:
| 2021-07-20 08:11:54 | INFO | Train Loss: 0.221, tp: 7, fn: 0, fp: 8, tn: 49, Acc: 0.875, Prec: 0.467, Rec: 1.000, F1: 0.780
| 2021-07-20 08:13:01 | INFO | Validation tp: 28, fn: 113, fp: 55, tn: 804
| 2021-07-20 08:13:01 | INFO | Validation loss: 0.488, acc: 0.832, F1: 0.578
| 2021-07-20 08:13:01 | INFO | Start epoch 10:
| 2021-07-20 08:13:01 | INFO | Train Loss: 0.139, tp: 11, fn: 3, fp: 1, tn: 49, Acc: 0.938, Prec: 0.917, Rec: 0.786, F1: 0.903
| 2021-07-20 08:14:08 | INFO | Validation tp: 29, fn: 112, fp: 43, tn: 816
| 2021-07-20 08:14:08 | INFO | Validation loss: 0.448, acc: 0.845, F1: 0.593
| 2021-07-20 08:14:08 | INFO | Start epoch 11:
| 2021-07-20 08:14:09 | INFO | Train Loss: 0.280, tp: 9, fn: 5, fp: 1, tn: 49, Acc: 0.906, Prec: 0.900, Rec: 0.643, F1: 0.846
| 2021-07-20 08:15:16 | INFO | Validation tp: 28, fn: 113, fp: 51, tn: 808
| 2021-07-20 08:15:16 | INFO | Validation loss: 0.622, acc: 0.836, F1: 0.581
| 2021-07-20 08:15:16 | INFO | Start epoch 12:
| 2021-07-20 08:15:17 | INFO | Train Loss: 0.020, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:16:23 | INFO | Validation tp: 24, fn: 117, fp: 54, tn: 805
| 2021-07-20 08:16:23 | INFO | Validation loss: 0.641, acc: 0.829, F1: 0.562
| 2021-07-20 08:16:23 | INFO | Start epoch 13:
| 2021-07-20 08:16:24 | INFO | Train Loss: 0.027, tp: 11, fn: 0, fp: 1, tn: 52, Acc: 0.984, Prec: 0.917, Rec: 1.000, F1: 0.973
| 2021-07-20 08:17:31 | INFO | Validation tp: 28, fn: 113, fp: 52, tn: 807
| 2021-07-20 08:17:31 | INFO | Validation loss: 0.765, acc: 0.835, F1: 0.580
| 2021-07-20 08:17:31 | INFO | Start epoch 14:
| 2021-07-20 08:17:31 | INFO | Train Loss: 0.083, tp: 8, fn: 2, fp: 0, tn: 54, Acc: 0.969, Prec: 1.000, Rec: 0.800, F1: 0.935
| 2021-07-20 08:18:38 | INFO | Validation tp: 24, fn: 117, fp: 51, tn: 808
| 2021-07-20 08:18:38 | INFO | Validation loss: 0.779, acc: 0.832, F1: 0.564
| 2021-07-20 08:18:38 | INFO | Start epoch 15:
| 2021-07-20 08:18:39 | INFO | Train Loss: 0.006, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:19:46 | INFO | Validation tp: 24, fn: 117, fp: 47, tn: 812
| 2021-07-20 08:19:46 | INFO | Validation loss: 0.834, acc: 0.836, F1: 0.567
| 2021-07-20 08:19:46 | INFO | Start epoch 16:
| 2021-07-20 08:19:46 | INFO | Train Loss: 0.011, tp: 5, fn: 0, fp: 0, tn: 59, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:20:53 | INFO | Validation tp: 29, fn: 112, fp: 59, tn: 800
| 2021-07-20 08:20:53 | INFO | Validation loss: 0.892, acc: 0.829, F1: 0.578
| 2021-07-20 08:20:53 | INFO | Start epoch 17:
| 2021-07-20 08:20:54 | INFO | Train Loss: 0.004, tp: 17, fn: 0, fp: 0, tn: 47, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:22:01 | INFO | Validation tp: 19, fn: 122, fp: 38, tn: 821
| 2021-07-20 08:22:01 | INFO | Validation loss: 0.882, acc: 0.840, F1: 0.552
| 2021-07-20 08:22:01 | INFO | Start epoch 18:
| 2021-07-20 08:22:01 | INFO | Train Loss: 0.003, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:23:08 | INFO | Validation tp: 27, fn: 114, fp: 63, tn: 796
| 2021-07-20 08:23:08 | INFO | Validation loss: 0.959, acc: 0.823, F1: 0.567
| 2021-07-20 08:23:08 | INFO | Start epoch 19:
| 2021-07-20 08:23:09 | INFO | Train Loss: 0.003, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:24:16 | INFO | Validation tp: 23, fn: 118, fp: 57, tn: 802
| 2021-07-20 08:24:16 | INFO | Validation loss: 0.974, acc: 0.825, F1: 0.555
| 2021-07-20 08:24:16 | INFO | Start epoch 20:
| 2021-07-20 08:24:16 | INFO | Train Loss: 0.004, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:25:24 | INFO | Validation tp: 24, fn: 117, fp: 55, tn: 804
| 2021-07-20 08:25:24 | INFO | Validation loss: 0.976, acc: 0.828, F1: 0.561
| 2021-07-20 08:25:24 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 08:25:26 | INFO | 
==============================Start training==============================
| 2021-07-20 08:25:26 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.3 --num_epochs 20 -c config/enzh_ner.conf
Config File (config/enzh_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enzh_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 08:25:26 | INFO | 
lr: 4e-05

| 2021-07-20 08:25:40 | INFO | Start epoch 1:
| 2021-07-20 08:25:41 | INFO | Train Loss: 0.711, tp: 9, fn: 2, fp: 41, tn: 12, Acc: 0.328, Prec: 0.180, Rec: 0.818, F1: 0.327
| 2021-07-20 08:26:47 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:26:47 | INFO | Validation loss: 0.397, acc: 0.859, F1: 0.462
| 2021-07-20 08:26:47 | INFO | Start epoch 2:
| 2021-07-20 08:26:48 | INFO | Train Loss: 0.375, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:27:55 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:27:55 | INFO | Validation loss: 0.387, acc: 0.859, F1: 0.462
| 2021-07-20 08:27:55 | INFO | Start epoch 3:
| 2021-07-20 08:27:56 | INFO | Train Loss: 0.388, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:29:03 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:29:03 | INFO | Validation loss: 0.377, acc: 0.859, F1: 0.462
| 2021-07-20 08:29:03 | INFO | Start epoch 4:
| 2021-07-20 08:29:03 | INFO | Train Loss: 0.481, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:30:10 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:30:10 | INFO | Validation loss: 0.378, acc: 0.859, F1: 0.462
| 2021-07-20 08:30:10 | INFO | Start epoch 5:
| 2021-07-20 08:30:10 | INFO | Train Loss: 0.507, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:31:17 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:31:17 | INFO | Validation loss: 0.377, acc: 0.859, F1: 0.462
| 2021-07-20 08:31:17 | INFO | Start epoch 6:
| 2021-07-20 08:31:18 | INFO | Train Loss: 0.371, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:32:25 | INFO | Validation tp: 0, fn: 141, fp: 0, tn: 859
| 2021-07-20 08:32:25 | INFO | Validation loss: 0.372, acc: 0.859, F1: 0.462
| 2021-07-20 08:32:25 | INFO | Start epoch 7:
| 2021-07-20 08:32:25 | INFO | Train Loss: 0.351, tp: 0, fn: 12, fp: 0, tn: 52, Acc: 0.812, Prec: 0.000, Rec: 0.000, F1: 0.448
| 2021-07-20 08:33:32 | INFO | Validation tp: 19, fn: 122, fp: 24, tn: 835
| 2021-07-20 08:33:32 | INFO | Validation loss: 0.396, acc: 0.854, F1: 0.563
| 2021-07-20 08:33:32 | INFO | Start epoch 8:
| 2021-07-20 08:33:33 | INFO | Train Loss: 0.472, tp: 2, fn: 14, fp: 2, tn: 46, Acc: 0.750, Prec: 0.500, Rec: 0.125, F1: 0.526
| 2021-07-20 08:34:40 | INFO | Validation tp: 39, fn: 102, fp: 64, tn: 795
| 2021-07-20 08:34:40 | INFO | Validation loss: 0.443, acc: 0.834, F1: 0.613
| 2021-07-20 08:34:40 | INFO | Start epoch 9:
| 2021-07-20 08:34:40 | INFO | Train Loss: 0.143, tp: 6, fn: 1, fp: 1, tn: 56, Acc: 0.969, Prec: 0.857, Rec: 0.857, F1: 0.920
| 2021-07-20 08:35:47 | INFO | Validation tp: 23, fn: 118, fp: 46, tn: 813
| 2021-07-20 08:35:47 | INFO | Validation loss: 0.458, acc: 0.836, F1: 0.564
| 2021-07-20 08:35:47 | INFO | Start epoch 10:
| 2021-07-20 08:35:48 | INFO | Train Loss: 0.204, tp: 10, fn: 4, fp: 1, tn: 49, Acc: 0.922, Prec: 0.909, Rec: 0.714, F1: 0.876
| 2021-07-20 08:36:55 | INFO | Validation tp: 21, fn: 120, fp: 43, tn: 816
| 2021-07-20 08:36:55 | INFO | Validation loss: 0.504, acc: 0.837, F1: 0.557
| 2021-07-20 08:36:55 | INFO | Start epoch 11:
| 2021-07-20 08:36:56 | INFO | Train Loss: 0.371, tp: 8, fn: 6, fp: 0, tn: 50, Acc: 0.906, Prec: 1.000, Rec: 0.571, F1: 0.835
| 2021-07-20 08:38:03 | INFO | Validation tp: 31, fn: 110, fp: 53, tn: 806
| 2021-07-20 08:38:03 | INFO | Validation loss: 0.561, acc: 0.837, F1: 0.592
| 2021-07-20 08:38:03 | INFO | Start epoch 12:
| 2021-07-20 08:38:03 | INFO | Train Loss: 0.050, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:39:10 | INFO | Validation tp: 38, fn: 103, fp: 89, tn: 770
| 2021-07-20 08:39:10 | INFO | Validation loss: 0.669, acc: 0.808, F1: 0.586
| 2021-07-20 08:39:10 | INFO | Start epoch 13:
| 2021-07-20 08:39:11 | INFO | Train Loss: 0.080, tp: 11, fn: 0, fp: 2, tn: 51, Acc: 0.969, Prec: 0.846, Rec: 1.000, F1: 0.949
| 2021-07-20 08:40:19 | INFO | Validation tp: 27, fn: 114, fp: 48, tn: 811
| 2021-07-20 08:40:19 | INFO | Validation loss: 0.755, acc: 0.838, F1: 0.580
| 2021-07-20 08:40:19 | INFO | Start epoch 14:
| 2021-07-20 08:40:19 | INFO | Train Loss: 0.065, tp: 9, fn: 1, fp: 0, tn: 54, Acc: 0.984, Prec: 1.000, Rec: 0.900, F1: 0.969
| 2021-07-20 08:41:26 | INFO | Validation tp: 22, fn: 119, fp: 54, tn: 805
| 2021-07-20 08:41:26 | INFO | Validation loss: 0.769, acc: 0.827, F1: 0.553
| 2021-07-20 08:41:26 | INFO | Start epoch 15:
| 2021-07-20 08:41:26 | INFO | Train Loss: 0.007, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:42:33 | INFO | Validation tp: 43, fn: 98, fp: 84, tn: 775
| 2021-07-20 08:42:33 | INFO | Validation loss: 0.901, acc: 0.818, F1: 0.608
| 2021-07-20 08:42:33 | INFO | Start epoch 16:
| 2021-07-20 08:42:34 | INFO | Train Loss: 0.003, tp: 5, fn: 0, fp: 0, tn: 59, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:43:41 | INFO | Validation tp: 34, fn: 107, fp: 67, tn: 792
| 2021-07-20 08:43:41 | INFO | Validation loss: 0.866, acc: 0.826, F1: 0.591
| 2021-07-20 08:43:41 | INFO | Start epoch 17:
| 2021-07-20 08:43:41 | INFO | Train Loss: 0.007, tp: 17, fn: 0, fp: 0, tn: 47, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:44:48 | INFO | Validation tp: 27, fn: 114, fp: 61, tn: 798
| 2021-07-20 08:44:48 | INFO | Validation loss: 0.931, acc: 0.825, F1: 0.568
| 2021-07-20 08:44:48 | INFO | Start epoch 18:
| 2021-07-20 08:44:48 | INFO | Train Loss: 0.002, tp: 14, fn: 0, fp: 0, tn: 50, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:45:55 | INFO | Validation tp: 22, fn: 119, fp: 48, tn: 811
| 2021-07-20 08:45:55 | INFO | Validation loss: 0.939, acc: 0.833, F1: 0.558
| 2021-07-20 08:45:55 | INFO | Start epoch 19:
| 2021-07-20 08:45:56 | INFO | Train Loss: 0.002, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:47:02 | INFO | Validation tp: 33, fn: 108, fp: 73, tn: 786
| 2021-07-20 08:47:02 | INFO | Validation loss: 0.970, acc: 0.819, F1: 0.582
| 2021-07-20 08:47:02 | INFO | Start epoch 20:
| 2021-07-20 08:47:03 | INFO | Train Loss: 0.002, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 08:48:10 | INFO | Validation tp: 23, fn: 118, fp: 54, tn: 805
| 2021-07-20 08:48:10 | INFO | Validation loss: 0.970, acc: 0.828, F1: 0.557
| 2021-07-20 08:48:10 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 08:48:13 | INFO | 
==============================Start training==============================
| 2021-07-20 08:48:13 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.2 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 08:48:13 | INFO | 
lr: 2e-05

| 2021-07-20 08:48:26 | INFO | Start epoch 1:
| 2021-07-20 08:48:26 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 08:49:40 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:49:40 | INFO | Validation loss: 0.330, acc: 0.904, F1: 0.475
| 2021-07-20 08:49:40 | INFO | Start epoch 2:
| 2021-07-20 08:49:41 | INFO | Train Loss: 0.237, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 08:50:56 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:50:56 | INFO | Validation loss: 0.320, acc: 0.904, F1: 0.475
| 2021-07-20 08:50:56 | INFO | Start epoch 3:
| 2021-07-20 08:50:57 | INFO | Train Loss: 0.188, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 08:52:12 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:52:12 | INFO | Validation loss: 0.308, acc: 0.904, F1: 0.475
| 2021-07-20 08:52:12 | INFO | Start epoch 4:
| 2021-07-20 08:52:12 | INFO | Train Loss: 0.302, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 08:53:27 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:53:27 | INFO | Validation loss: 0.295, acc: 0.904, F1: 0.475
| 2021-07-20 08:53:27 | INFO | Start epoch 5:
| 2021-07-20 08:53:27 | INFO | Train Loss: 0.286, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:54:42 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:54:42 | INFO | Validation loss: 0.330, acc: 0.904, F1: 0.475
| 2021-07-20 08:54:42 | INFO | Start epoch 6:
| 2021-07-20 08:54:42 | INFO | Train Loss: 0.349, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 08:55:58 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:55:58 | INFO | Validation loss: 0.342, acc: 0.904, F1: 0.475
| 2021-07-20 08:55:58 | INFO | Start epoch 7:
| 2021-07-20 08:55:58 | INFO | Train Loss: 0.462, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-20 08:57:13 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:57:13 | INFO | Validation loss: 0.323, acc: 0.904, F1: 0.475
| 2021-07-20 08:57:13 | INFO | Start epoch 8:
| 2021-07-20 08:57:13 | INFO | Train Loss: 0.356, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:58:28 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:58:28 | INFO | Validation loss: 0.294, acc: 0.904, F1: 0.475
| 2021-07-20 08:58:28 | INFO | Start epoch 9:
| 2021-07-20 08:58:29 | INFO | Train Loss: 0.308, tp: 0, fn: 7, fp: 1, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 08:59:44 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 08:59:44 | INFO | Validation loss: 0.296, acc: 0.904, F1: 0.475
| 2021-07-20 08:59:44 | INFO | Start epoch 10:
| 2021-07-20 08:59:44 | INFO | Train Loss: 0.245, tp: 0, fn: 5, fp: 0, tn: 59, Acc: 0.922, Prec: 0.000, Rec: 0.000, F1: 0.480
| 2021-07-20 09:00:59 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:00:59 | INFO | Validation loss: 0.298, acc: 0.904, F1: 0.475
| 2021-07-20 09:00:59 | INFO | Start epoch 11:
| 2021-07-20 09:01:00 | INFO | Train Loss: 0.229, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 09:02:15 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:02:15 | INFO | Validation loss: 0.316, acc: 0.904, F1: 0.475
| 2021-07-20 09:02:15 | INFO | Start epoch 12:
| 2021-07-20 09:02:16 | INFO | Train Loss: 0.346, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:03:30 | INFO | Validation tp: 1, fn: 95, fp: 3, tn: 901
| 2021-07-20 09:03:30 | INFO | Validation loss: 0.310, acc: 0.902, F1: 0.484
| 2021-07-20 09:03:30 | INFO | Start epoch 13:
| 2021-07-20 09:03:31 | INFO | Train Loss: 0.176, tp: 0, fn: 4, fp: 0, tn: 60, Acc: 0.938, Prec: 0.000, Rec: 0.000, F1: 0.484
| 2021-07-20 09:04:45 | INFO | Validation tp: 2, fn: 94, fp: 6, tn: 898
| 2021-07-20 09:04:45 | INFO | Validation loss: 0.318, acc: 0.900, F1: 0.493
| 2021-07-20 09:04:45 | INFO | Start epoch 14:
| 2021-07-20 09:04:46 | INFO | Train Loss: 0.330, tp: 0, fn: 7, fp: 1, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:06:00 | INFO | Validation tp: 1, fn: 95, fp: 8, tn: 896
| 2021-07-20 09:06:00 | INFO | Validation loss: 0.324, acc: 0.897, F1: 0.482
| 2021-07-20 09:06:00 | INFO | Start epoch 15:
| 2021-07-20 09:06:01 | INFO | Train Loss: 0.249, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 09:07:16 | INFO | Validation tp: 2, fn: 94, fp: 8, tn: 896
| 2021-07-20 09:07:16 | INFO | Validation loss: 0.319, acc: 0.898, F1: 0.492
| 2021-07-20 09:07:16 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 09:07:19 | INFO | 
==============================Start training==============================
| 2021-07-20 09:07:19 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.2 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 09:07:19 | INFO | 
lr: 3e-05

| 2021-07-20 09:07:32 | INFO | Start epoch 1:
| 2021-07-20 09:07:33 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 09:08:47 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:08:47 | INFO | Validation loss: 0.331, acc: 0.904, F1: 0.475
| 2021-07-20 09:08:47 | INFO | Start epoch 2:
| 2021-07-20 09:08:48 | INFO | Train Loss: 0.222, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 09:10:02 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:10:02 | INFO | Validation loss: 0.311, acc: 0.904, F1: 0.475
| 2021-07-20 09:10:02 | INFO | Start epoch 3:
| 2021-07-20 09:10:03 | INFO | Train Loss: 0.174, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 09:11:17 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:11:17 | INFO | Validation loss: 0.294, acc: 0.904, F1: 0.475
| 2021-07-20 09:11:17 | INFO | Start epoch 4:
| 2021-07-20 09:11:18 | INFO | Train Loss: 0.318, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 09:12:33 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:12:33 | INFO | Validation loss: 0.292, acc: 0.904, F1: 0.475
| 2021-07-20 09:12:33 | INFO | Start epoch 5:
| 2021-07-20 09:12:33 | INFO | Train Loss: 0.289, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:13:47 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:13:47 | INFO | Validation loss: 0.291, acc: 0.904, F1: 0.475
| 2021-07-20 09:13:47 | INFO | Start epoch 6:
| 2021-07-20 09:13:48 | INFO | Train Loss: 0.314, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 09:15:02 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:15:02 | INFO | Validation loss: 0.322, acc: 0.904, F1: 0.475
| 2021-07-20 09:15:02 | INFO | Start epoch 7:
| 2021-07-20 09:15:03 | INFO | Train Loss: 0.303, tp: 1, fn: 9, fp: 0, tn: 54, Acc: 0.859, Prec: 1.000, Rec: 0.100, F1: 0.552
| 2021-07-20 09:16:18 | INFO | Validation tp: 10, fn: 86, fp: 19, tn: 885
| 2021-07-20 09:16:18 | INFO | Validation loss: 0.349, acc: 0.895, F1: 0.552
| 2021-07-20 09:16:18 | INFO | Start epoch 8:
| 2021-07-20 09:16:18 | INFO | Train Loss: 0.140, tp: 4, fn: 4, fp: 1, tn: 55, Acc: 0.922, Prec: 0.800, Rec: 0.500, F1: 0.786
| 2021-07-20 09:17:33 | INFO | Validation tp: 20, fn: 76, fp: 67, tn: 837
| 2021-07-20 09:17:33 | INFO | Validation loss: 0.388, acc: 0.857, F1: 0.570
| 2021-07-20 09:17:33 | INFO | Start epoch 9:
| 2021-07-20 09:17:34 | INFO | Train Loss: 0.099, tp: 6, fn: 1, fp: 3, tn: 54, Acc: 0.938, Prec: 0.667, Rec: 0.857, F1: 0.857
| 2021-07-20 09:18:48 | INFO | Validation tp: 12, fn: 84, fp: 33, tn: 871
| 2021-07-20 09:18:48 | INFO | Validation loss: 0.467, acc: 0.883, F1: 0.554
| 2021-07-20 09:18:48 | INFO | Start epoch 10:
| 2021-07-20 09:18:49 | INFO | Train Loss: 0.039, tp: 4, fn: 1, fp: 0, tn: 59, Acc: 0.984, Prec: 1.000, Rec: 0.800, F1: 0.940
| 2021-07-20 09:20:04 | INFO | Validation tp: 24, fn: 72, fp: 68, tn: 836
| 2021-07-20 09:20:04 | INFO | Validation loss: 0.507, acc: 0.860, F1: 0.589
| 2021-07-20 09:20:04 | INFO | Start epoch 11:
| 2021-07-20 09:20:04 | INFO | Train Loss: 0.140, tp: 5, fn: 1, fp: 2, tn: 56, Acc: 0.953, Prec: 0.714, Rec: 0.833, F1: 0.872
| 2021-07-20 09:21:18 | INFO | Validation tp: 15, fn: 81, fp: 29, tn: 875
| 2021-07-20 09:21:18 | INFO | Validation loss: 0.511, acc: 0.890, F1: 0.578
| 2021-07-20 09:21:18 | INFO | Start epoch 12:
| 2021-07-20 09:21:19 | INFO | Train Loss: 0.011, tp: 8, fn: 0, fp: 0, tn: 56, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:22:34 | INFO | Validation tp: 8, fn: 88, fp: 28, tn: 876
| 2021-07-20 09:22:34 | INFO | Validation loss: 0.612, acc: 0.884, F1: 0.530
| 2021-07-20 09:22:34 | INFO | Start epoch 13:
| 2021-07-20 09:22:35 | INFO | Train Loss: 0.094, tp: 2, fn: 2, fp: 0, tn: 60, Acc: 0.969, Prec: 1.000, Rec: 0.500, F1: 0.825
| 2021-07-20 09:23:50 | INFO | Validation tp: 12, fn: 84, fp: 38, tn: 866
| 2021-07-20 09:23:50 | INFO | Validation loss: 0.580, acc: 0.878, F1: 0.549
| 2021-07-20 09:23:50 | INFO | Start epoch 14:
| 2021-07-20 09:23:50 | INFO | Train Loss: 0.004, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:25:05 | INFO | Validation tp: 13, fn: 83, fp: 43, tn: 861
| 2021-07-20 09:25:05 | INFO | Validation loss: 0.609, acc: 0.874, F1: 0.551
| 2021-07-20 09:25:05 | INFO | Start epoch 15:
| 2021-07-20 09:25:05 | INFO | Train Loss: 0.004, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:26:21 | INFO | Validation tp: 12, fn: 84, fp: 36, tn: 868
| 2021-07-20 09:26:21 | INFO | Validation loss: 0.601, acc: 0.880, F1: 0.551
| 2021-07-20 09:26:21 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 09:26:23 | INFO | 
==============================Start training==============================
| 2021-07-20 09:26:23 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.2 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 09:26:23 | INFO | 
lr: 4e-05

| 2021-07-20 09:26:37 | INFO | Start epoch 1:
| 2021-07-20 09:26:37 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 09:27:51 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:27:51 | INFO | Validation loss: 0.324, acc: 0.904, F1: 0.475
| 2021-07-20 09:27:51 | INFO | Start epoch 2:
| 2021-07-20 09:27:52 | INFO | Train Loss: 0.217, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 09:29:07 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:29:07 | INFO | Validation loss: 0.301, acc: 0.904, F1: 0.475
| 2021-07-20 09:29:07 | INFO | Start epoch 3:
| 2021-07-20 09:29:07 | INFO | Train Loss: 0.179, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 09:30:22 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:30:22 | INFO | Validation loss: 0.296, acc: 0.904, F1: 0.475
| 2021-07-20 09:30:22 | INFO | Start epoch 4:
| 2021-07-20 09:30:23 | INFO | Train Loss: 0.285, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 09:31:38 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:31:38 | INFO | Validation loss: 0.301, acc: 0.904, F1: 0.475
| 2021-07-20 09:31:38 | INFO | Start epoch 5:
| 2021-07-20 09:31:38 | INFO | Train Loss: 0.299, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:32:53 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:32:53 | INFO | Validation loss: 0.303, acc: 0.904, F1: 0.475
| 2021-07-20 09:32:53 | INFO | Start epoch 6:
| 2021-07-20 09:32:54 | INFO | Train Loss: 0.285, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 09:34:08 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:34:08 | INFO | Validation loss: 0.313, acc: 0.904, F1: 0.475
| 2021-07-20 09:34:08 | INFO | Start epoch 7:
| 2021-07-20 09:34:09 | INFO | Train Loss: 0.309, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-20 09:35:23 | INFO | Validation tp: 9, fn: 87, fp: 28, tn: 876
| 2021-07-20 09:35:23 | INFO | Validation loss: 0.354, acc: 0.885, F1: 0.537
| 2021-07-20 09:35:23 | INFO | Start epoch 8:
| 2021-07-20 09:35:24 | INFO | Train Loss: 0.123, tp: 6, fn: 2, fp: 1, tn: 55, Acc: 0.953, Prec: 0.857, Rec: 0.750, F1: 0.887
| 2021-07-20 09:36:38 | INFO | Validation tp: 31, fn: 65, fp: 92, tn: 812
| 2021-07-20 09:36:38 | INFO | Validation loss: 0.429, acc: 0.843, F1: 0.597
| 2021-07-20 09:36:38 | INFO | Start epoch 9:
| 2021-07-20 09:36:39 | INFO | Train Loss: 0.105, tp: 7, fn: 0, fp: 2, tn: 55, Acc: 0.969, Prec: 0.778, Rec: 1.000, F1: 0.929
| 2021-07-20 09:37:53 | INFO | Validation tp: 8, fn: 88, fp: 25, tn: 879
| 2021-07-20 09:37:53 | INFO | Validation loss: 0.445, acc: 0.887, F1: 0.532
| 2021-07-20 09:37:53 | INFO | Start epoch 10:
| 2021-07-20 09:37:54 | INFO | Train Loss: 0.024, tp: 5, fn: 0, fp: 0, tn: 59, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:39:08 | INFO | Validation tp: 10, fn: 86, fp: 40, tn: 864
| 2021-07-20 09:39:08 | INFO | Validation loss: 0.489, acc: 0.874, F1: 0.535
| 2021-07-20 09:39:08 | INFO | Start epoch 11:
| 2021-07-20 09:39:09 | INFO | Train Loss: 0.012, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:40:24 | INFO | Validation tp: 7, fn: 89, fp: 21, tn: 883
| 2021-07-20 09:40:24 | INFO | Validation loss: 0.533, acc: 0.890, F1: 0.527
| 2021-07-20 09:40:24 | INFO | Start epoch 12:
| 2021-07-20 09:40:24 | INFO | Train Loss: 0.089, tp: 7, fn: 1, fp: 0, tn: 56, Acc: 0.984, Prec: 1.000, Rec: 0.875, F1: 0.962
| 2021-07-20 09:41:39 | INFO | Validation tp: 11, fn: 85, fp: 32, tn: 872
| 2021-07-20 09:41:39 | INFO | Validation loss: 0.559, acc: 0.883, F1: 0.548
| 2021-07-20 09:41:39 | INFO | Start epoch 13:
| 2021-07-20 09:41:40 | INFO | Train Loss: 0.019, tp: 3, fn: 1, fp: 0, tn: 60, Acc: 0.984, Prec: 1.000, Rec: 0.750, F1: 0.924
| 2021-07-20 09:42:54 | INFO | Validation tp: 11, fn: 85, fp: 33, tn: 871
| 2021-07-20 09:42:54 | INFO | Validation loss: 0.605, acc: 0.882, F1: 0.547
| 2021-07-20 09:42:54 | INFO | Start epoch 14:
| 2021-07-20 09:42:55 | INFO | Train Loss: 0.004, tp: 7, fn: 0, fp: 0, tn: 57, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:44:10 | INFO | Validation tp: 11, fn: 85, fp: 36, tn: 868
| 2021-07-20 09:44:10 | INFO | Validation loss: 0.641, acc: 0.879, F1: 0.544
| 2021-07-20 09:44:10 | INFO | Start epoch 15:
| 2021-07-20 09:44:10 | INFO | Train Loss: 0.004, tp: 6, fn: 0, fp: 0, tn: 58, Acc: 1.000, Prec: 1.000, Rec: 1.000, F1: 1.000
| 2021-07-20 09:45:25 | INFO | Validation tp: 10, fn: 86, fp: 31, tn: 873
| 2021-07-20 09:45:25 | INFO | Validation loss: 0.627, acc: 0.883, F1: 0.542
| 2021-07-20 09:45:25 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 09:45:27 | INFO | 
==============================Start training==============================
| 2021-07-20 09:45:27 | INFO | Command Line Args:   --lr 2e-5 --warmup_ratio 0.3 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 09:45:27 | INFO | 
lr: 2e-05

| 2021-07-20 09:45:39 | INFO | Start epoch 1:
| 2021-07-20 09:45:40 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 09:46:54 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:46:54 | INFO | Validation loss: 0.327, acc: 0.904, F1: 0.475
| 2021-07-20 09:46:54 | INFO | Start epoch 2:
| 2021-07-20 09:46:55 | INFO | Train Loss: 0.253, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 09:48:10 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:48:10 | INFO | Validation loss: 0.311, acc: 0.904, F1: 0.475
| 2021-07-20 09:48:10 | INFO | Start epoch 3:
| 2021-07-20 09:48:10 | INFO | Train Loss: 0.201, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 09:49:25 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:49:25 | INFO | Validation loss: 0.291, acc: 0.904, F1: 0.475
| 2021-07-20 09:49:25 | INFO | Start epoch 4:
| 2021-07-20 09:49:26 | INFO | Train Loss: 0.313, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 09:50:40 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:50:40 | INFO | Validation loss: 0.294, acc: 0.904, F1: 0.475
| 2021-07-20 09:50:40 | INFO | Start epoch 5:
| 2021-07-20 09:50:41 | INFO | Train Loss: 0.356, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:51:56 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:51:56 | INFO | Validation loss: 0.296, acc: 0.904, F1: 0.475
| 2021-07-20 09:51:56 | INFO | Start epoch 6:
| 2021-07-20 09:51:57 | INFO | Train Loss: 0.366, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 09:53:11 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:53:11 | INFO | Validation loss: 0.297, acc: 0.904, F1: 0.475
| 2021-07-20 09:53:11 | INFO | Start epoch 7:
| 2021-07-20 09:53:11 | INFO | Train Loss: 0.413, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-20 09:54:26 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 09:54:26 | INFO | Validation loss: 0.303, acc: 0.904, F1: 0.475
| 2021-07-20 09:54:26 | INFO | Start epoch 8:
| 2021-07-20 09:54:27 | INFO | Train Loss: 0.241, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 09:55:41 | INFO | Validation tp: 18, fn: 78, fp: 22, tn: 882
| 2021-07-20 09:55:41 | INFO | Validation loss: 0.309, acc: 0.900, F1: 0.606
| 2021-07-20 09:55:41 | INFO | Start epoch 9:
| 2021-07-20 09:55:42 | INFO | Train Loss: 0.245, tp: 1, fn: 6, fp: 1, tn: 56, Acc: 0.891, Prec: 0.500, Rec: 0.143, F1: 0.582
| 2021-07-20 09:56:57 | INFO | Validation tp: 12, fn: 84, fp: 17, tn: 887
| 2021-07-20 09:56:57 | INFO | Validation loss: 0.340, acc: 0.899, F1: 0.569
| 2021-07-20 09:56:57 | INFO | Start epoch 10:
| 2021-07-20 09:56:57 | INFO | Train Loss: 0.143, tp: 3, fn: 2, fp: 1, tn: 58, Acc: 0.953, Prec: 0.750, Rec: 0.600, F1: 0.821
| 2021-07-20 09:58:12 | INFO | Validation tp: 9, fn: 87, fp: 36, tn: 868
| 2021-07-20 09:58:12 | INFO | Validation loss: 0.436, acc: 0.877, F1: 0.531
| 2021-07-20 09:58:12 | INFO | Start epoch 11:
| 2021-07-20 09:58:12 | INFO | Train Loss: 0.200, tp: 3, fn: 3, fp: 1, tn: 57, Acc: 0.938, Prec: 0.750, Rec: 0.500, F1: 0.783
| 2021-07-20 09:59:27 | INFO | Validation tp: 13, fn: 83, fp: 56, tn: 848
| 2021-07-20 09:59:27 | INFO | Validation loss: 0.485, acc: 0.861, F1: 0.541
| 2021-07-20 09:59:27 | INFO | Start epoch 12:
| 2021-07-20 09:59:28 | INFO | Train Loss: 0.158, tp: 6, fn: 2, fp: 1, tn: 55, Acc: 0.953, Prec: 0.857, Rec: 0.750, F1: 0.887
| 2021-07-20 10:00:43 | INFO | Validation tp: 12, fn: 84, fp: 47, tn: 857
| 2021-07-20 10:00:43 | INFO | Validation loss: 0.535, acc: 0.869, F1: 0.542
| 2021-07-20 10:00:43 | INFO | Start epoch 13:
| 2021-07-20 10:00:44 | INFO | Train Loss: 0.053, tp: 4, fn: 0, fp: 1, tn: 59, Acc: 0.984, Prec: 0.800, Rec: 1.000, F1: 0.940
| 2021-07-20 10:01:59 | INFO | Validation tp: 17, fn: 79, fp: 60, tn: 844
| 2021-07-20 10:01:59 | INFO | Validation loss: 0.537, acc: 0.861, F1: 0.560
| 2021-07-20 10:01:59 | INFO | Start epoch 14:
| 2021-07-20 10:01:59 | INFO | Train Loss: 0.098, tp: 6, fn: 1, fp: 2, tn: 55, Acc: 0.953, Prec: 0.750, Rec: 0.857, F1: 0.887
| 2021-07-20 10:03:14 | INFO | Validation tp: 12, fn: 84, fp: 49, tn: 855
| 2021-07-20 10:03:14 | INFO | Validation loss: 0.559, acc: 0.867, F1: 0.540
| 2021-07-20 10:03:14 | INFO | Start epoch 15:
| 2021-07-20 10:03:15 | INFO | Train Loss: 0.042, tp: 6, fn: 0, fp: 1, tn: 57, Acc: 0.984, Prec: 0.857, Rec: 1.000, F1: 0.957
| 2021-07-20 10:04:30 | INFO | Validation tp: 11, fn: 85, fp: 33, tn: 871
| 2021-07-20 10:04:30 | INFO | Validation loss: 0.551, acc: 0.882, F1: 0.547
| 2021-07-20 10:04:30 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 10:04:33 | INFO | 
==============================Start training==============================
| 2021-07-20 10:04:33 | INFO | Command Line Args:   --lr 3e-5 --warmup_ratio 0.3 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 10:04:33 | INFO | 
lr: 3e-05

| 2021-07-20 10:04:46 | INFO | Start epoch 1:
| 2021-07-20 10:04:46 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 10:06:01 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:06:01 | INFO | Validation loss: 0.330, acc: 0.904, F1: 0.475
| 2021-07-20 10:06:01 | INFO | Start epoch 2:
| 2021-07-20 10:06:02 | INFO | Train Loss: 0.237, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 10:07:18 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:07:18 | INFO | Validation loss: 0.320, acc: 0.904, F1: 0.475
| 2021-07-20 10:07:18 | INFO | Start epoch 3:
| 2021-07-20 10:07:18 | INFO | Train Loss: 0.188, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 10:08:34 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:08:34 | INFO | Validation loss: 0.308, acc: 0.904, F1: 0.475
| 2021-07-20 10:08:34 | INFO | Start epoch 4:
| 2021-07-20 10:08:34 | INFO | Train Loss: 0.302, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 10:09:49 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:09:49 | INFO | Validation loss: 0.321, acc: 0.904, F1: 0.475
| 2021-07-20 10:09:49 | INFO | Start epoch 5:
| 2021-07-20 10:09:50 | INFO | Train Loss: 0.402, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 10:11:05 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:11:05 | INFO | Validation loss: 0.319, acc: 0.904, F1: 0.475
| 2021-07-20 10:11:05 | INFO | Start epoch 6:
| 2021-07-20 10:11:05 | INFO | Train Loss: 0.353, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 10:12:21 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:12:21 | INFO | Validation loss: 0.306, acc: 0.904, F1: 0.475
| 2021-07-20 10:12:21 | INFO | Start epoch 7:
| 2021-07-20 10:12:21 | INFO | Train Loss: 0.432, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
| 2021-07-20 10:26:49 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:26:49 | INFO | Validation loss: 0.301, acc: 0.904, F1: 0.475
| 2021-07-20 10:26:49 | INFO | Start epoch 8:
| 2021-07-20 10:26:50 | INFO | Train Loss: 0.341, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 10:28:03 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:28:03 | INFO | Validation loss: 0.305, acc: 0.904, F1: 0.475
| 2021-07-20 10:28:03 | INFO | Start epoch 9:
| 2021-07-20 10:28:04 | INFO | Train Loss: 0.296, tp: 0, fn: 7, fp: 1, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 10:29:18 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:29:18 | INFO | Validation loss: 0.305, acc: 0.904, F1: 0.475
| 2021-07-20 10:29:18 | INFO | Start epoch 10:
| 2021-07-20 10:29:19 | INFO | Train Loss: 0.259, tp: 0, fn: 5, fp: 0, tn: 59, Acc: 0.922, Prec: 0.000, Rec: 0.000, F1: 0.480
| 2021-07-20 10:30:32 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:30:32 | INFO | Validation loss: 0.311, acc: 0.904, F1: 0.475
| 2021-07-20 10:30:32 | INFO | Start epoch 11:
| 2021-07-20 10:30:33 | INFO | Train Loss: 0.180, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 10:31:47 | INFO | Validation tp: 6, fn: 90, fp: 13, tn: 891
| 2021-07-20 10:31:47 | INFO | Validation loss: 0.383, acc: 0.897, F1: 0.525
| 2021-07-20 10:31:47 | INFO | Start epoch 12:
| 2021-07-20 10:31:48 | INFO | Train Loss: 0.129, tp: 3, fn: 5, fp: 0, tn: 56, Acc: 0.922, Prec: 1.000, Rec: 0.375, F1: 0.751
| 2021-07-20 10:33:01 | INFO | Validation tp: 6, fn: 90, fp: 18, tn: 886
| 2021-07-20 10:33:01 | INFO | Validation loss: 0.457, acc: 0.892, F1: 0.521
| 2021-07-20 10:33:01 | INFO | Start epoch 13:
| 2021-07-20 10:33:02 | INFO | Train Loss: 0.042, tp: 4, fn: 0, fp: 2, tn: 58, Acc: 0.969, Prec: 0.667, Rec: 1.000, F1: 0.892
| 2021-07-20 10:34:16 | INFO | Validation tp: 17, fn: 79, fp: 45, tn: 859
| 2021-07-20 10:34:16 | INFO | Validation loss: 0.475, acc: 0.876, F1: 0.574
| 2021-07-20 10:34:16 | INFO | Start epoch 14:
| 2021-07-20 10:34:16 | INFO | Train Loss: 0.077, tp: 6, fn: 1, fp: 2, tn: 55, Acc: 0.953, Prec: 0.750, Rec: 0.857, F1: 0.887
| 2021-07-20 10:35:30 | INFO | Validation tp: 11, fn: 85, fp: 37, tn: 867
| 2021-07-20 10:35:30 | INFO | Validation loss: 0.559, acc: 0.878, F1: 0.544
| 2021-07-20 10:35:30 | INFO | Start epoch 15:
| 2021-07-20 10:35:31 | INFO | Train Loss: 0.027, tp: 6, fn: 0, fp: 1, tn: 57, Acc: 0.984, Prec: 0.857, Rec: 1.000, F1: 0.957
| 2021-07-20 10:36:45 | INFO | Validation tp: 12, fn: 84, fp: 37, tn: 867
| 2021-07-20 10:36:45 | INFO | Validation loss: 0.560, acc: 0.879, F1: 0.550
| 2021-07-20 10:36:45 | INFO | MonoTransQuestModel(
  (model): XLMRobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): RobertaPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.1, inplace=False)
    (1): Linear(in_features=772, out_features=772, bias=True)
    (2): Tanh()
    (3): Dropout(p=0.1, inplace=False)
    (4): Linear(in_features=772, out_features=2, bias=True)
  )
)
| 2021-07-20 10:36:47 | INFO | 
==============================Start training==============================
| 2021-07-20 10:36:47 | INFO | Command Line Args:   --lr 4e-5 --warmup_ratio 0.3 -c config/enja_ner.conf
Config File (config/enja_ner.conf):
  train_data:        wmt21_official_data/data_with_NER_feature/enja_majority_train_ner.tsv
  valid_data:        wmt21_official_data/data_with_NER_feature/enja_majority_dev_ner.tsv
  output_dir:        output/temp/
  huggingface_model: xlm-roberta-base
  max_sequence_length:100
  dropout:           0.1
  num_epochs:        15
  train_batch_size:  64
  valid_batch_size:  16
Defaults:
  --hidden_size:     256
  --ner:             False
  --tox:             False

| 2021-07-20 10:36:47 | INFO | 
lr: 4e-05

| 2021-07-20 10:36:59 | INFO | Start epoch 1:
| 2021-07-20 10:36:59 | INFO | Train Loss: 0.710, tp: 4, fn: 0, fp: 41, tn: 19, Acc: 0.359, Prec: 0.089, Rec: 1.000, F1: 0.322
| 2021-07-20 10:38:14 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:38:14 | INFO | Validation loss: 0.340, acc: 0.904, F1: 0.475
| 2021-07-20 10:38:14 | INFO | Start epoch 2:
| 2021-07-20 10:38:14 | INFO | Train Loss: 0.229, tp: 0, fn: 3, fp: 0, tn: 61, Acc: 0.953, Prec: 0.000, Rec: 0.000, F1: 0.488
| 2021-07-20 10:39:29 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:39:29 | INFO | Validation loss: 0.319, acc: 0.904, F1: 0.475
| 2021-07-20 10:39:29 | INFO | Start epoch 3:
| 2021-07-20 10:39:30 | INFO | Train Loss: 0.173, tp: 0, fn: 2, fp: 0, tn: 62, Acc: 0.969, Prec: 0.000, Rec: 0.000, F1: 0.492
| 2021-07-20 10:40:44 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:40:44 | INFO | Validation loss: 0.305, acc: 0.904, F1: 0.475
| 2021-07-20 10:40:44 | INFO | Start epoch 4:
| 2021-07-20 10:40:45 | INFO | Train Loss: 0.315, tp: 0, fn: 6, fp: 0, tn: 58, Acc: 0.906, Prec: 0.000, Rec: 0.000, F1: 0.475
| 2021-07-20 10:42:00 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:42:00 | INFO | Validation loss: 0.306, acc: 0.904, F1: 0.475
| 2021-07-20 10:42:00 | INFO | Start epoch 5:
| 2021-07-20 10:42:01 | INFO | Train Loss: 0.368, tp: 0, fn: 8, fp: 0, tn: 56, Acc: 0.875, Prec: 0.000, Rec: 0.000, F1: 0.467
| 2021-07-20 10:43:16 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:43:16 | INFO | Validation loss: 0.303, acc: 0.904, F1: 0.475
| 2021-07-20 10:43:16 | INFO | Start epoch 6:
| 2021-07-20 10:43:16 | INFO | Train Loss: 0.334, tp: 0, fn: 7, fp: 0, tn: 57, Acc: 0.891, Prec: 0.000, Rec: 0.000, F1: 0.471
| 2021-07-20 10:44:31 | INFO | Validation tp: 0, fn: 96, fp: 0, tn: 904
| 2021-07-20 10:44:31 | INFO | Validation loss: 0.305, acc: 0.904, F1: 0.475
| 2021-07-20 10:44:31 | INFO | Start epoch 7:
| 2021-07-20 10:44:32 | INFO | Train Loss: 0.460, tp: 0, fn: 10, fp: 0, tn: 54, Acc: 0.844, Prec: 0.000, Rec: 0.000, F1: 0.458
